<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>deep learning on felladog</title><link>https://utsavdarlami.github.io/blogs/categories/deep-learning/</link><description>Recent content in deep learning on felladog</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Wed, 26 May 2021 17:16:00 +0545</lastBuildDate><atom:link href="https://utsavdarlami.github.io/blogs/categories/deep-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>neural network</title><link>https://utsavdarlami.github.io/blogs/notes/2021-05-26--11-31-31z--neural_network/</link><pubDate>Wed, 26 May 2021 17:16:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/2021-05-26--11-31-31z--neural_network/</guid><description>References : Done Reading: Sebastian Raschka Perceptron Lecture Slide Reading: Tom Mitchell Lectures slides of chapter 4 Tom Mitchell, Machine Learning Chapter 4 To Read: https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/ https://zzsza.github.io/data/2018/05/13/cs231n-backpropagation-and-neural-networks/ Questions : Biological Motivation Mimicing human brain Figure 1: biological neuron vs aritifical neuron from this post properties that match with the human brain Many neuron like threshold switching units Many weighted interconnections among units Highly parallel, distributed process Emphasis on tuning weights automatically Representations The ANNs can graphs be with many types of structures: acyclic or cyclic directed or undirected The backprop algorithm assumes ANN to have structure that corresponds to a directed graph, and possibly containing cyclees.</description></item><item><title>Batch Normalization</title><link>https://utsavdarlami.github.io/blogs/notes/2021-03-06--02-21-33z--batch_normalization/</link><pubDate>Sat, 06 Mar 2021 08:06:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/2021-03-06--02-21-33z--batch_normalization/</guid><description>Acknowledgement : d2l.ai batch normalization L11.2 How BatchNorm Works - Sebastian Raschka https://twitter.com/karpathy/status/1013245864570073090 To read: Why is batch norm becoming so unpopular Questions : What is Normalization? Normalization A popular and effective technique that constantly accelerates the convergence of deep network. Normalizes hidden layer inputs. Helps with exploding/vanishing gradient problems. Can increase the training stability and convergence rate.</description></item><item><title>Recurrent Neural Networks</title><link>https://utsavdarlami.github.io/blogs/notes/20210213135051-recurrent_neural_networks/</link><pubDate>Tue, 02 Mar 2021 12:40:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/20210213135051-recurrent_neural_networks/</guid><description>Acknowledgement https://d2l.ai/chapter%5Frecurrent-neural-networks/index.html
https://github.com/fastai/fastbook/blob/master/12%5Fnlp%5Fdive.ipynb
https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks
https://colah.github.io/posts/2015-08-Understanding-LSTMs/ [Mainly LSTM]
https://deeplearning.cs.cmu.edu/S20/document/recitation/recitation-7.pdf
http://cs231n.stanford.edu/slides/2017/cs231n%5F2017%5Flecture10.pdf
https://www.youtube.com/watch?v=6niqTuYFZLQ
https://github.com/fastai/fastbook/blob/master/12%5Fnlp%5Fdive.ipynb [ fast ai nlp dive rnn archi]
http://ethen8181.github.io/machine-learning/deep%5Flearning/rnn/1%5Fpytorch%5Frnn.html#Recurrent-Neural-Network-(RNN) [Main reference]
https://medium.com/ecovisioneth/building-deep-multi-layer-recurrent-neural-networks-with-star-cell-2f01acdb73a7 [Multi Layer]
https://towardsdatascience.com/pytorch-basics-how-to-train-your-neural-net-intro-to-rnn-cb6ebc594677
https://www.jeremyjordan.me/introduction-to-recurrent-neural-networks/
https://towardsdatascience.com/pytorch-basics-how-to-train-your-neural-net-intro-to-rnn-cb6ebc594677
Sebastian Raschka Character Generation using lstm cell pytorch
Recurrent neural networks (RNNs) are designed to better handle sequential information.</description></item></channel></rss>