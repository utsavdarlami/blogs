<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>NLP on felladog</title><link>https://utsavdarlami.github.io/blogs/categories/nlp/</link><description>Recent content in NLP on felladog</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Wed, 31 Mar 2021 12:57:00 +0545</lastBuildDate><atom:link href="https://utsavdarlami.github.io/blogs/categories/nlp/index.xml" rel="self" type="application/rss+xml"/><item><title>text-preprocessing</title><link>https://utsavdarlami.github.io/blogs/notes/2021-03-31-07-12-59z-text_preprocessing/</link><pubDate>Wed, 31 Mar 2021 12:57:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/2021-03-31-07-12-59z-text_preprocessing/</guid><description>References : [Book] Builiding the ML systems with python http://tfidf.com/ Questions : Bag of Words Bag of word approach, it totally ignores the order of words and simply uses word counts as their basis. For each word in the post, its occurrence is counted and noted in a vector. Not surprisingly, this step is also called vectorization. The vector is typically huge as it contains as many elements as words occur in the whole dataset.</description></item></channel></rss>