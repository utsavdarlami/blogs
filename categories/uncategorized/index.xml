<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>uncategorized on felladog</title><link>https://utsavdarlami.github.io/blogs/categories/uncategorized/</link><description>Recent content in uncategorized on felladog</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Thu, 03 Jun 2021 16:52:00 +0545</lastBuildDate><atom:link href="https://utsavdarlami.github.io/blogs/categories/uncategorized/index.xml" rel="self" type="application/rss+xml"/><item><title>Inductive Bias</title><link>https://utsavdarlami.github.io/blogs/notes/2021-06-03-11-07-01z-inductive_bias/</link><pubDate>Thu, 03 Jun 2021 16:52:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/2021-06-03-11-07-01z-inductive_bias/</guid><description>References :
To Read: https://en.wikipedia.org/wiki/Inductive%5Fbias Questions :
The inductive bias of a learning algorithm is the set of assumptions that the learner uses to predict outputs given inputs that it has not encountered.</description></item><item><title>decision tree</title><link>https://utsavdarlami.github.io/blogs/notes/2021-06-03-11-01-27z-decision_tree/</link><pubDate>Thu, 03 Jun 2021 16:46:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/2021-06-03-11-01-27z-decision_tree/</guid><description> References :
Questions :</description></item><item><title>perceptron</title><link>https://utsavdarlami.github.io/blogs/notes/20210224173929-perceptron/</link><pubDate>Wed, 26 May 2021 17:16:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/20210224173929-perceptron/</guid><description>References :
Done Reading: Sebastian Raschka Perceptron Lecture Slide Reading: Tom Mitchell Lectures slides of chapter 4 Tom Mitchell, Machine Learning Chapter 4 To Read: https://www.wikiwand.com/en/Perceptron Questions :
A learning rule for the computational/mathematical neuron model Rosenblatt, F. (1957). The perceptron, a perceiving and recognizing automaton. Project Para. Cornell Aeronautical Laboratory Figure 1: A perceptron \(o(\overrightarrow{x}) = sgn(\overrightarrow{w} \cdot \overrightarrow{x})\) where, \( sgn(y) = \begin{cases} 1 &amp;amp; \text{if y &amp;gt; 0} \\ -1 &amp;amp; \text{otherwise} \end{cases} \) Space \(H\) of candidate hypotheses is \(H = \{ \overrightarrow{w} | \overrightarrow{w} \in \Re^{n+1} \}\) Representations A single layer perceptron can be used to represent many boolean functions AND \(w_{0}\) = -.</description></item><item><title>activation function</title><link>https://utsavdarlami.github.io/blogs/notes/2021-05-22-10-42-02z-activation_function/</link><pubDate>Sat, 22 May 2021 16:27:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/2021-05-22-10-42-02z-activation_function/</guid><description>References : Reading: Tom Mitchell Lectures slides of chapter 4 Tom Mitchell, Machine Learning Chapter 4 Questions : Also known as squashing Function Figure 1: Activation Functions, from this post Sigmoid output is a non linear function of its inputs and is differentiable threshold function also called logistic function output ranges from 0 to 1 Figure 2: The sigmoid threshold unit, from Tom Mitchell Lectures Tanh \(f(x) = tanh(x) = \frac{(e^{x} - e^{-x})} {(e^{x} + e^{-x})}\)</description></item></channel></rss>