<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>uncategorized on felladog</title><link>https://utsavdarlami.github.io/blogs/categories/uncategorized/</link><description>Recent content in uncategorized on felladog</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Thu, 27 May 2021 11:05:00 +0545</lastBuildDate><atom:link href="https://utsavdarlami.github.io/blogs/categories/uncategorized/index.xml" rel="self" type="application/rss+xml"/><item><title>image thresholding</title><link>https://utsavdarlami.github.io/blogs/notes/2021-05-27--05-20-06z--thresholding/</link><pubDate>Thu, 27 May 2021 11:05:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/2021-05-27--05-20-06z--thresholding/</guid><description>References :
https://muthu.co/otsus-method-for-image-thresholding-explained-and-implemented/ Questions :
Can be used for image segmentation
multi-otsu thresholding The multi-Otsu threshold is a thresholding algorithm that is used to separate the pixels of an input image into several different classes, each one obtained according to the intensity of the gray levels within the image Multi-Otsu calculates several thresholds based on the number supplied by the user. for multi classes import numpy as np import matplotlib.</description></item><item><title>perceptron</title><link>https://utsavdarlami.github.io/blogs/notes/20210224173929-perceptron/</link><pubDate>Wed, 26 May 2021 17:16:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/20210224173929-perceptron/</guid><description>References :
Done Reading: Sebastian Raschka Perceptron Lecture Slide Reading: Tom Mitchell Lectures slides of chapter 4 Tom Mitchell, Machine Learning Chapter 4 To Read: https://www.wikiwand.com/en/Perceptron Questions :
A learning rule for the computational/mathematical neuron model Rosenblatt, F. (1957). The perceptron, a perceiving and recognizing automaton. Project Para. Cornell Aeronautical Laboratory Figure 1: A perceptron \(o(\overrightarrow{x}) = sgn(\overrightarrow{w} \cdot \overrightarrow{x})\) where, \( sgn(y) = \begin{cases} 1 &amp;amp; \text{if y &amp;gt; 0} \\ -1 &amp;amp; \text{otherwise} \end{cases} \) Space \(H\) of candidate hypotheses is \(H = \{ \overrightarrow{w} | \overrightarrow{w} \in \Re^{n+1} \}\) Representations A single layer perceptron can be used to represent many boolean functions AND \(w_{0}\) = -.</description></item><item><title>activation function</title><link>https://utsavdarlami.github.io/blogs/notes/2021-05-22--10-42-02z--activation_function/</link><pubDate>Sat, 22 May 2021 16:27:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/2021-05-22--10-42-02z--activation_function/</guid><description>References : Reading: Tom Mitchell Lectures slides of chapter 4 Tom Mitchell, Machine Learning Chapter 4 Questions : Also known as squashing Function Figure 1: Activation Functions, from this post Sigmoid output is a non linear function of its inputs and is differentiable threshold function also called logistic function output ranges from 0 to 1 Figure 2: The sigmoid threshold unit, from Tom Mitchell Lectures Tanh \(f(x) = tanh(x) = \frac{(e^{x} - e^{-x})} {(e^{x} + e^{-x})}\)</description></item></channel></rss>