+++
title = "entropy"
author = ["felladog"]
date = 2021-03-06T09:41:00+05:45
tags = ["entropy"]
categories = ["machine learning"]
draft = false
+++

---

-   References :

    -   <https://www.wikiwand.com/en/Entropy%5F(information%5Ftheory)>
    -   [Entropy in image, scikit-image](https://scikit-image.org/docs/stable/auto%5Fexamples/filters/plot%5Fentropy.html#sphx-glr-auto-examples-filters-plot-entropy-py)

    [3] <https://bricaud.github.io/personal-blog/entropy-in-decision-trees/>
-   Questions :
    -   What is the point of entropy in decision tree?

---

**Entropy is a measure of disorder.**
A high entropy is essentially saying that the data is scattered around while a low entropy means that nearly all the data is the same.
The highest entropy one can achieve is 1 and lowest entropy one can achieve is 0.

-   In information theory, information entropy is the log-base-2 of the number of possible outcomes for a message.
    -   \\(- \log\_{2} p\\)

-   Based on our dataset we can say
    -   It is an indicator of how messy your data is.
    -   Characterizes the (im)purity of an arbitary collection of examples.

-   Given a discrete random variable X, with possible outcomes \\(x\_{1},...,x\_{n}\\) which occur with probability \\( {P} (x\_{1}),..., {P} (x\_{n}), \\) the entropy of X is formally defined as:
    -   \\( H(X) = - \sum\_{i=1}^{n} P(x\_i) log\_2 P(x\_i) \\) ,


## What is the point of entropy in decision tree? {#what-is-the-point-of-entropy-in-decision-tree}

_At each step, each branching, you want to decrease the entropy, so this quantity is computed before the cut and after the cut. If it decreases, the split is validated and we can proceed to the next step, otherwise, we must try to split with another feature or stop this branch._[3]


## Application {#application}

-   In [image filtering]({{<relref "2021-05-28--12-02-59Z--image_convolution.md" >}}) entropy is used for texture analysis
-   In decision tree
