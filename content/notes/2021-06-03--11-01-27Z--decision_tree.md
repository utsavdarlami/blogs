+++
title = "decision tree"
author = ["felladog"]
date = 2021-06-03T16:46:00+05:45
lastmod = 2021-06-06T22:20:10+05:45
tags = ["entropy", "ID3", "information gain"]
categories = ["machine learning"]
draft = false
+++

---

-   References :
    -   Reading :
        -   Tom Mitchell Lectures slides of chapter 3
        -   Tom Mitchell, Machine Learning Chapter 3
        -   ID3 algorithm complete solution
            -   [Reddit Post Link](https://www.reddit.com/r/learnmachinelearning/comments/nmsdhs/i%5Fsolved%5Fa%5Fid3%5Falgorithm%5Fin%5Fmachine%5Flearning/)
            -   [Drive Pdf Link](https://drive.google.com/file/d/1b2ocYqBxl3oX2o0S-dLgbVVNSGGamUel/view)
    -   To Read :
        -   [Chapter 3 — Decision Tree Learning — Part 1](https://medium.com/@pralhad2481/chapter-3-decision-tree-learning-part-1-d0ca2365bb22)

-   Questions :

---

-   Approximating discrete-valued fucntions
-   robust to noisy data
-   capable of learning disjunctive expressions
-   search a completely expressive hypothesis space and thus avoid the difficulties of restricted hypothesis spaces.


## Representation {#representation}

-   Decision trees classify instances by sorting them down the tree from the root to some leaf node
-   each leaf node assigns a classification
-   each interal node specifies a test of some attributes of the instances
-   each branch descending from a node corresponds to one of the possible values of the attribute represented by that node.

{{< figure src="/ox-hugo/dt_1.png" >}}

-   For the instance <Outlook = Sunny, Temperature = Hot, Humidity = High, Wind=Strong>
    -   PlayTennis = no
-   In general, decision trees represent a disjunction of conjunctions of constraints on the attribute values of instances.
-   Each path from the tree root to a leaf corresponds to a conjunction of attribute tests, and the tree itself to a disjunction of these conjunctions.


## When is Decision Tree suitable ? {#when-is-decision-tree-suitable}

1.  Instances are represented by many attribute value pairs.
2.  The target function output may be
    -   discrete-valued,
    -   with certain extension can be used for real-valued output
3.  The training examples may contain noises/errors
4.  Disjunctive descriptions may be required
5.  The training data may contain missing attribute values


## ID3 {#id3}

-   Basic algorithm for learning decision tree.
-   "Which attribute should be tested at the root of the tree?"
    -   Which attribute to test at each node in the tree


### [entropy]({{<relref "2021-03-06--03-56-06Z--entropy.md" >}}) {#entropy--2021-03-06-03-56-06z-entropy-dot-md}

-   Given a collection 5, containing positive and negative examples of some target concept, the entropy of S relative to this boolean classification is
    -   \\( Entropy(S) = - p\_{\oplus} log\_2 p\_{\oplus} - p\_{\ominus} log\_2 p\_{\ominus} \\)
    -   where, \\(p\_{\oplus}\\) is the proportion of positive examples in S and,
    -   \\(p\_{\ominus}\\) is the proportion of negative examples in S.
-   Example,
    -   Entropy([9+, 5-]) = 0.940
-   Entropy is
    -   0 if all members of S belong to the same class.
    -   1 when the collection contains an equal number of positive and negative examples.

{{< figure src="/ox-hugo/dt_ent1.png" >}}

-   If the target attribute can take on c different values, then the entropy of S relative to this c-wise classification is defined as
    -   \\( Entropy(S) = \sum\_{i=1}^{c} - p\_i log\_2 p\_i \\) , where \\(p\_i\\), is the proportion of S belonging to class i.


### Information gain {#information-gain}

-   measures how well a given attribute separates the training examples according to their target classification.
-   ID3 uses it to select attributes among the candidate attributes at each step while growing the tree.
-   It is the expected reduction in the entropy caused by partitioning the examples based on the attributes
-   the information gain, Gain(S, A) of an attribute A, relative to a collection of examples S, is defined as
    -   \\( Gain(S,A) = Entropy(S) - \sum\_{v \in Values(A)} \frac{|S\_v|}{|S|} Entropy(S\_v)  \\)
    -   where, Values(A) is the set of all possible values for attribute A,
    -   \\(S\_v\\) is the subset of S for which attribute A has value v (i.e., \\( S\_v = \\{s \in S|A(s) = v\\}) \\).
    -   the first term is just the entropy of the original collection S,
    -   and the second term is the expected value of the entropy after S is partitioned using attribute A.
-   Gain(S, A) is
    -   the information provided about the target function value, given the value of some other attribute A.
    -   the number of bits saved when encoding the target value of an arbitrary member of S, by knowing the value of attribute A.


### Illustrative Example {#illustrative-example}

{{< figure src="/ox-hugo/dt_eg.png" >}}

{{< figure src="/ox-hugo/dt_root.png" >}}

-   The information gain values for all four attributes are
    -   Gain(S, Outlook) = 0.246
    -   Gain(S, Humidity) = 0.151
    -   Gain(S, Wind) = 0.048
    -   Gain(S, Temperature) = 0.029

{{< figure src="/ox-hugo/dt_final_eg.png" >}}

-   ID3 algorithm complete solution
    -   [Reddit Post Link](https://www.reddit.com/r/learnmachinelearning/comments/nmsdhs/i%5Fsolved%5Fa%5Fid3%5Falgorithm%5Fin%5Fmachine%5Flearning/)


## Hypothesis Space Search {#hypothesis-space-search}

-   ID3 performs a simple-to-complex, hill-climbing search through hypothesis space,
    -   beginning with the empty tree,
    -   then considering progressively more elaborate hypotheses in search of decision tree that correctly classifies the training data.
-   hill-climbing search is guided information gain measure.

{{< figure src="/ox-hugo/id3_search.png" >}}


### Limitations and Capabilities of ID3 {#limitations-and-capabilities-of-id3}

-   ID3's hypothesis space of all decision trees is a complete space of finite discrete-valued functions, relative to the available attributes.
    -   Thus, avoids one of the major risks of methods that search incomplete hypothesis spaces(such as methods that consider only conjunctive hypotheses): that the hypothesis space might not contain the target function.
-   Maintains only a single current hypothesis as it searches through the space of decision trees.
    -   wheras, [Candidate-Elimination Learning Algorithm]({{<relref "2021-06-04--14-08-26Z--concept_learning.md" >}}), which maintains the set of all hypotheses consistent with the available training examples.
    -   it does not have the ability to determine how many alternative decision trees are consistent with the available training data,
-   ID3 in its pure form performs no backtracking in its search.
    -   usually risks of hill-climbing search without backtracking: converging to locally optimal solutions that are not globally optimal.
-   ID3 uses all training examples at each step in the search to make statistically based decisions regarding how to refine its current hypothesis.
    -   contrasts with methods that make decisions incrementally, based on individual training examples (e.g., [Find-S]({{<relref "2021-06-04--14-08-26Z--concept_learning.md" >}}) or [Candidate-Elimination Learning Algorithm]({{<relref "2021-06-04--14-08-26Z--concept_learning.md" >}}))
    -   Advantage is that the resulting search is much less sensitive to errors in individual training examples.


## [Inductive Bias]({{<relref "2021-06-03--11-07-01Z--inductive_bias.md" >}}) {#inductive-bias--2021-06-03-11-07-01z-inductive-bias-dot-md}

-   the ID3 search strategy
    1.  selects in favor of shorter trees over longer ones, and
    2.  selects trees that place the attributes with highest information gain closest to the root.

-   **Approximate inductive bias of ID3**:
    -   Shorter trees are preferred over larger trees.
-   BFS-ID3, Breadth First Search for ID3
-   **A closer approximation to the inductive bias of ID3**:
    -   Shorter trees are preferred over longer trees. Trees that place high information gain attributes close to the root are preferred over those that do not.


## Restriction and Preference Biases {#restriction-and-preference-biases}

-   the inductive bias of
    -   ID3 follows from its **search strategy**,
    -   Candidate-Elimination algorithm follows from the definition of its **search space**.
-   ID3 exhibits a purely preference bias, and
-   Candidate-Elimination purely restriction bias.


## Why prefer Short Hypotheses {#why-prefer-short-hypotheses}

-   **Occam's razor**: Prefer the simplest hypothesis that fits the data.
-   Why? (Argument in Favor)
    -   One argument is that because there are fewer short hypotheses than long ones, it is less likely that one will find a short hypothesis that coincidentally fits the training data.
    -   In contrast there are often many very complex hypotheses that fit the current training data but fail to generalize correctly to subsequent data.
-   Argument Opposed
    -   There are many ways to define small sets of hypothesis


## Issues {#issues}
