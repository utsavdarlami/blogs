+++
title = "perceptron"
author = ["felladog"]
date = 2021-05-26T17:16:00+05:45
lastmod = 2021-06-03T20:14:43+05:45
tags = ["ml", "deep learning"]
categories = ["uncategorized"]
draft = false
+++

---

-   References :

    -   Done Reading:
        -   [Sebastian Raschka Perceptron Lecture Slide](https://github.com/rasbt/stat453-deep-learning-ss20/blob/master/L03-perceptron/L03%5Fperceptron%5Fslides.pdf)

    <!--listend-->

    -   Reading:
        -   Tom Mitchell Lectures slides of chapter 4
        -   Tom Mitchell, Machine Learning Chapter 4
    -   To Read:
        -   <https://www.wikiwand.com/en/Perceptron>
-   Questions :

---

-   A learning rule for the computational/mathematical neuron model
-   Rosenblatt, F. (1957). The perceptron, a perceiving and recognizing automaton. Project Para. Cornell Aeronautical Laboratory

{{< figure src="/ox-hugo/2021-06-02_21-10-22_screenshot.png" caption="Figure 1: A perceptron" width="600" height="400" target="/blogs" >}}

-   \\(o(\overrightarrow{x}) = sgn(\overrightarrow{w} \cdot \overrightarrow{x})\\) where,
-   \\( sgn(y) = \begin{cases} 1 & \text{if y > 0} \\\ -1 & \text{otherwise} \end{cases} \\)
-   Space \\(H\\) of candidate hypotheses is
    -   \\(H = \\{ \overrightarrow{w} | \overrightarrow{w} \in \Re^{n+1} \\}\\)


## Representations {#representations}

-   A single layer perceptron can be used to represent many boolean functions
    -   AND
        -   \\(w\_{0}\\) = -.8
        -   \\(w\_{1}\\) = \\(w\_{2}\\)  = .5
    -   OR
        -   \\(w\_{0}\\) = -.3
        -   \\(w\_{1}\\) = \\(w\_{2}\\)  = .5
    -   NAND
    -   NOR
-   However XOR function cannot be represented by a single layer perceptron
    -   Since the single layer cannot linearly separate the training examples
    -   `In 1969, a famous book entitled Perceptrons by Marvin Minsky and Seymour Papert showed that it was impossible for these classes of network to learn an XOR function. It is often believed (incorrectly) that they also conjectured that a similar result would hold for a multi-layer perceptron network. However, this is not true, as both Minsky and Papert already knew that multi-layer perceptrons were capable of producing an XOR function. (See the page on Perceptrons (book) for more information.) Nevertheless, the often-miscited Minsky/Papert text caused a significant decline in interest and funding of neural network research. It took ten more years until neural network research experienced a resurgence in the 1980s` - [Perceptron](https://www.wikiwand.com/en/Perceptron)

{{< figure src="/ox-hugo/2021-06-02_21-22-44_screenshot.png" caption="Figure 2: XOR function" width="300" height="300" target="/blogs" >}}


## Learning Rule {#learning-rule}

{{< figure src="/ox-hugo/2021-06-02_22-03-29_screenshot.png" caption="Figure 3: Learning Rule, from Tom Mitchell Lectures" width="600" height="300" target="/blogs" >}}

-   converges only if (gauranteed to succeed):
    -   the data is linearly seperable
    -   the \\(\eta\\) (eta) is sufficiently small
-   Solution is [Gradient Descent And the Delta Rule]({{<relref "2021-05-26--11-31-31Z--neural_network.md" >}})


## Geometric Intuition {#geometric-intuition}

{{< figure src="/ox-hugo/2021-06-03_11-24-26_screenshot.png" caption="Figure 4: Geometirx Intuition, from Sebastian Raschka Lectures" width="550" height="400" target="/blogs" >}}
