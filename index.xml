<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>felladog blogs on felladog</title><link>https://utsavdarlami.github.io/blogs/</link><description>Recent content in felladog blogs on felladog</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sun, 03 Mar 2024 21:33:07 +0545</lastBuildDate><atom:link href="https://utsavdarlami.github.io/blogs/index.xml" rel="self" type="application/rss+xml"/><item><title>image binarization and pixel count</title><link>https://utsavdarlami.github.io/blogs/posts/20230829193605-image_binarization_and_pixel_count/</link><pubDate>Tue, 29 Aug 2023 19:36:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/posts/20230829193605-image_binarization_and_pixel_count/</guid><description>References : https://haesleinhuepf.github.io/BioImageAnalysisNotebooks/intro.html Questions : what is thresholding and binarization how can we calculate the relative area of certain object in the image. One of the critical aspects of image analysis involves quantifying pixel data, a process that&amp;rsquo;s integral to various scientific and medical applications. In this blog, we&amp;rsquo;ll try to explore the process of converting microscopic images into binary representations for pixel counting.
Steps involved Conversion to Gray Images First, raw microscopic images are converted to grayscale images.</description></item><item><title>concept learning</title><link>https://utsavdarlami.github.io/blogs/notes/2021-06-04--14-08-26z--concept_learning/</link><pubDate>Fri, 04 Jun 2021 19:53:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/2021-06-04--14-08-26z--concept_learning/</guid><description>References : Reading: Tom Mitchell Lectures slides of chapter 2 Tom Mitchell, Machine Learning Chapter 2 To Read: Candidate elimination algorithm with an example Questions : Inferring boolean valued function from training examples of its input and outputs. A problem of searching through a predefined space of potential hypotheses for the hypothesis that best fits the training examples Example Traget concept or function: Concept or function to be learned. For our example - &amp;ldquo;days on which &amp;ldquo;A&amp;rdquo; can enjoy water sport&amp;rdquo;, EnjoySport Denoted by \(c\) c can be any boolean valued function defined over the instances X; \( c : X \rightarrow \{ 0,1 \} \) Instances The set of items over which the concept is defined.</description></item><item><title>Inductive Bias</title><link>https://utsavdarlami.github.io/blogs/notes/2021-06-03--11-07-01z--inductive_bias/</link><pubDate>Thu, 03 Jun 2021 16:52:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/2021-06-03--11-07-01z--inductive_bias/</guid><description>References :
Reading: Tom Mitchell Lectures slides of chapter 2 Tom Mitchell, Machine Learning Chapter 2 To Read: https://en.wikipedia.org/wiki/Inductive%5Fbias Questions :
The inductive bias of a learning algorithm is the set of assumptions that the learner uses to predict outputs given inputs that it has not encountered.
Consider a concept learning algorithm L for the set of instances X. Let c be an arbitrary concept defined over X, and let \(D_c\) = {&amp;lt;x,c(x)&amp;gt;} be an arbitrary set of training examples of c.</description></item><item><title>decision tree</title><link>https://utsavdarlami.github.io/blogs/notes/2021-06-03--11-01-27z--decision_tree/</link><pubDate>Thu, 03 Jun 2021 16:46:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/2021-06-03--11-01-27z--decision_tree/</guid><description>References :
Reading : Tom Mitchell Lectures slides of chapter 3 Tom Mitchell, Machine Learning Chapter 3 ID3 algorithm complete solution Reddit Post Link Drive Pdf Link To Read : Chapter 3 — Decision Tree Learning — Part 1 Questions :
Approximating discrete-valued fucntions robust to noisy data capable of learning disjunctive expressions search a completely expressive hypothesis space and thus avoid the difficulties of restricted hypothesis spaces. Representation Decision trees classify instances by sorting them down the tree from the root to some leaf node each leaf node assigns a classification each interal node specifies a test of some attributes of the instances each branch descending from a node corresponds to one of the possible values of the attribute represented by that node.</description></item><item><title>edge filters</title><link>https://utsavdarlami.github.io/blogs/notes/2021-05-31--03-54-38z--edge_filters/</link><pubDate>Mon, 31 May 2021 09:39:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/2021-05-31--03-54-38z--edge_filters/</guid><description>References :
Done Reading Reading https://nptel.ac.in/courses/106/106/106106224/ [IIT Madras Week 2 part 1] Cornell CV lecture on Edge https://szeliski.org/Book/ [Book Chapter 3] https://www.wikiwand.com/en/Sobel%5Foperator Joseph Redmon Lectures https://evergreenllc2020.medium.com/fundamentals-of-image-gradients-and-edge-detection-b093662ade1b To read: Questions :
Criteria for a good edge detector Good detection find all real edges, ignoring noise or other artifacts Good Localization detect edges as close as possible to true edges Single Response return one point only for each true edge point from skimage.</description></item><item><title>gradient</title><link>https://utsavdarlami.github.io/blogs/notes/2021-05-30--08-53-22z--gradient/</link><pubDate>Sun, 30 May 2021 14:38:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/2021-05-30--08-53-22z--gradient/</guid><description>References : Done Reading: Sebastian Raschka Gradient Descent Lecture Slide To Read: https://www.wikiwand.com/en/Gradient Questions : In vector calculus, the gradient is a multi-variable generalization of the derivative. Whereas the ordinary derivative of a function of a single variable is a scalar-valued function, the gradient of a function of several variables is a vector-valued function
Simply, derivative of multivariable function
The gradient (or gradient vector field) of a scalar function f(x1, x2, x3, …, xn) is denoted ∇f or ∇→f where ∇ (nabla) denotes the vector differential operator, del.</description></item><item><title>image filtering</title><link>https://utsavdarlami.github.io/blogs/notes/2021-05-28--12-02-59z--image_convolution/</link><pubDate>Fri, 28 May 2021 17:47:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/2021-05-28--12-02-59z--image_convolution/</guid><description>References : Done Reading Reading https://vincmazet.github.io/ftip/convolution.html https://nptel.ac.in/courses/106/106/106106224/ [IIT Madras] Cornell CV Lecture on Filter https://szeliski.org/Book/ [Book Chapter 3] Non-local means denoising for preserving textures To read: https://www.cs.tut.fi/~foi/papers/ICIP2019_Ymir.pdf [BM3D] https://stackoverflow.com/questions/40596026/what-does-entropy-mean-in-this-context Questions : Modify image pixels based on some function of a local neighbourhood of each pixel.
Why filter a image To sharpen, smooth, intensify or enhance a imag Linear filters Output pixel’s value is determined as a weighted sum of input pixel values within a small neighborhood N,</description></item><item><title>image thresholding</title><link>https://utsavdarlami.github.io/blogs/notes/2021-05-27--05-20-06z--thresholding/</link><pubDate>Thu, 27 May 2021 11:05:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/2021-05-27--05-20-06z--thresholding/</guid><description>References :
https://muthu.co/otsus-method-for-image-thresholding-explained-and-implemented/ Questions :
Can be used for image segmentation
multi-otsu thresholding The multi-Otsu threshold is a thresholding algorithm that is used to separate the pixels of an input image into several different classes, each one obtained according to the intensity of the gray levels within the image Multi-Otsu calculates several thresholds based on the number supplied by the user. for multi classes import numpy as np import matplotlib.</description></item><item><title>neural network</title><link>https://utsavdarlami.github.io/blogs/notes/2021-05-26--11-31-31z--neural_network/</link><pubDate>Wed, 26 May 2021 17:16:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/2021-05-26--11-31-31z--neural_network/</guid><description>References : Done Reading: Sebastian Raschka Perceptron Lecture Slide Reading: Tom Mitchell Lectures slides of chapter 4 Tom Mitchell, Machine Learning Chapter 4 To Read: https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/ https://zzsza.github.io/data/2018/05/13/cs231n-backpropagation-and-neural-networks/ Questions : Biological Motivation Mimicing human brain Figure 1: biological neuron vs aritifical neuron from this post properties that match with the human brain Many neuron like threshold switching units Many weighted interconnections among units Highly parallel, distributed process Emphasis on tuning weights automatically Representations The ANNs can graphs be with many types of structures: acyclic or cyclic directed or undirected The backprop algorithm assumes ANN to have structure that corresponds to a directed graph, and possibly containing cyclees.</description></item><item><title>perceptron</title><link>https://utsavdarlami.github.io/blogs/notes/20210224173929-perceptron/</link><pubDate>Wed, 26 May 2021 17:16:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/20210224173929-perceptron/</guid><description>References :
Done Reading: Sebastian Raschka Perceptron Lecture Slide Reading: Tom Mitchell Lectures slides of chapter 4 Tom Mitchell, Machine Learning Chapter 4 To Read: https://www.wikiwand.com/en/Perceptron Questions :
A learning rule for the computational/mathematical neuron model Rosenblatt, F. (1957). The perceptron, a perceiving and recognizing automaton. Project Para. Cornell Aeronautical Laboratory Figure 1: A perceptron \(o(\overrightarrow{x}) = sgn(\overrightarrow{w} \cdot \overrightarrow{x})\) where, \( sgn(y) = \begin{cases} 1 &amp;amp; \text{if y &amp;gt; 0} \\ -1 &amp;amp; \text{otherwise} \end{cases} \) Space \(H\) of candidate hypotheses is \(H = \{ \overrightarrow{w} | \overrightarrow{w} \in \Re^{n+1} \}\) Representations A single layer perceptron can be used to represent many boolean functions AND \(w_{0}\) = -.</description></item><item><title>regularization</title><link>https://utsavdarlami.github.io/blogs/notes/2021-05-26--11-28-39z--regularization/</link><pubDate>Wed, 26 May 2021 17:13:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/2021-05-26--11-28-39z--regularization/</guid><description>References :
To read: https://www.wikiwand.com/en/Regularization%5F(mathematics) https://www.wikiwand.com/en/Early%5Fstopping Questions :
In machine learning and regularization is the process of adding information in order to solve an ill-posed problem or to prevent overfitting.
Early Stopping Form of regularization used to avoid overfitting when training a learner with an iterative method, such as gradient descent. Such methods update the learner so as to make it better fit the training data with each iteration. Up to a point, this improves the learner&amp;rsquo;s performance on validation set.</description></item><item><title>activation function</title><link>https://utsavdarlami.github.io/blogs/notes/2021-05-22--10-42-02z--activation_function/</link><pubDate>Sat, 22 May 2021 16:27:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/2021-05-22--10-42-02z--activation_function/</guid><description>References : Reading: Tom Mitchell Lectures slides of chapter 4 Tom Mitchell, Machine Learning Chapter 4 Questions : Also known as squashing Function Figure 1: Activation Functions, from this post Sigmoid output is a non linear function of its inputs and is differentiable threshold function also called logistic function output ranges from 0 to 1 Figure 2: The sigmoid threshold unit, from Tom Mitchell Lectures Tanh \(f(x) = tanh(x) = \frac{(e^{x} - e^{-x})} {(e^{x} + e^{-x})}\)</description></item><item><title>text-preprocessing</title><link>https://utsavdarlami.github.io/blogs/notes/2021-03-31--07-12-59z--text_preprocessing/</link><pubDate>Wed, 31 Mar 2021 12:57:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/2021-03-31--07-12-59z--text_preprocessing/</guid><description>References : [Book] Builiding the ML systems with python http://tfidf.com/ Questions : Ways to preprocess text before Feature Extraction in NLP
When preprocessing, we can perform the following:
Eliminate handles and URLs Tokenize the string into words. Remove stop words like &amp;ldquo;and, is, a, on, etc.&amp;rdquo; Stemming or convert every word to its stem. Like dancer, dancing, danced, becomes &amp;lsquo;danc&amp;rsquo;. You can use porter stemmer to take care of this.</description></item><item><title>entropy</title><link>https://utsavdarlami.github.io/blogs/notes/2021-03-06--03-56-06z--entropy/</link><pubDate>Sat, 06 Mar 2021 09:41:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/2021-03-06--03-56-06z--entropy/</guid><description>References :
https://www.wikiwand.com/en/Entropy%5F(information%5Ftheory) Entropy in image, scikit-image [3] https://bricaud.github.io/personal-blog/entropy-in-decision-trees/
Questions :
What is the point of entropy in decision tree? Entropy is a measure of disorder. A high entropy is essentially saying that the data is scattered around while a low entropy means that nearly all the data is the same.
In information theory, information entropy is the log-base-2 of the number of possible outcomes for a message.
\(- \log_{2} p\) Based on our dataset we can say</description></item><item><title>Batch Normalization</title><link>https://utsavdarlami.github.io/blogs/notes/2021-03-06--02-21-33z--batch_normalization/</link><pubDate>Sat, 06 Mar 2021 08:06:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/2021-03-06--02-21-33z--batch_normalization/</guid><description>Acknowledgement : d2l.ai batch normalization L11.2 How BatchNorm Works - Sebastian Raschka https://twitter.com/karpathy/status/1013245864570073090 To read: Why is batch norm becoming so unpopular Questions : What is Normalization? Normalization A popular and effective technique that constantly accelerates the convergence of deep network. Normalizes hidden layer inputs. Helps with exploding/vanishing gradient problems. Can increase the training stability and convergence rate. Can be understood as additional(normalization) layers(with additional parameters). How is it done Batch normalization is applied to individual layers (optionally, to all of them) and works as follows:</description></item><item><title>Recurrent Neural Networks</title><link>https://utsavdarlami.github.io/blogs/notes/20210213135051-recurrent_neural_networks/</link><pubDate>Tue, 02 Mar 2021 12:40:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/20210213135051-recurrent_neural_networks/</guid><description> Acknowledgement https://d2l.ai/chapter%5Frecurrent-neural-networks/index.html
https://github.com/fastai/fastbook/blob/master/12%5Fnlp%5Fdive.ipynb
https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks
https://colah.github.io/posts/2015-08-Understanding-LSTMs/ [Mainly LSTM]
https://deeplearning.cs.cmu.edu/S20/document/recitation/recitation-7.pdf
http://cs231n.stanford.edu/slides/2017/cs231n%5F2017%5Flecture10.pdf
https://www.youtube.com/watch?v=6niqTuYFZLQ
https://github.com/fastai/fastbook/blob/master/12%5Fnlp%5Fdive.ipynb [ fast ai nlp dive rnn archi]
http://ethen8181.github.io/machine-learning/deep%5Flearning/rnn/1%5Fpytorch%5Frnn.html#Recurrent-Neural-Network-(RNN) [Main reference]
https://medium.com/ecovisioneth/building-deep-multi-layer-recurrent-neural-networks-with-star-cell-2f01acdb73a7 [Multi Layer]
https://towardsdatascience.com/pytorch-basics-how-to-train-your-neural-net-intro-to-rnn-cb6ebc594677
https://www.jeremyjordan.me/introduction-to-recurrent-neural-networks/
https://towardsdatascience.com/pytorch-basics-how-to-train-your-neural-net-intro-to-rnn-cb6ebc594677
Sebastian Raschka Character Generation using lstm cell pytorch
Recurrent neural networks (RNNs) are designed to better handle sequential information. (text and stocks)
RNNs introduce state variables(hidden state) to store past information, together with the current inputs, to determine the current outputs.
Figure 1: rnn vs nn Figure 2: a folded rnn Figure 3: a unfolded rnn Internal State of RNN Figure 4: a state diagram, Pytorch Implementation Summary</description></item><item><title>blog using org and hugo</title><link>https://utsavdarlami.github.io/blogs/posts/2021-02-28--11-34-36z--blog_using_org_and_hugo/</link><pubDate>Sun, 28 Feb 2021 17:19:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/posts/2021-02-28--11-34-36z--blog_using_org_and_hugo/</guid><description>References :
https://ddavis.io/ Copied shamelessly from here* https://seds.nl/ My ultimate plan https://github.com/jethrokuan/braindump https://github.com/jethrokuan/cortex [for backlinks] Questions :
I have made a base site using hugo static generator. The posts are written with org-mode. The plugin `ox-hugo` converts the org file to markdown and places it to the posts folder of the hugo site folder. I have also used roam capture template so that the placeholder for tags and categories are predefined in new org files.</description></item><item><title>ML Model Evaluation</title><link>https://utsavdarlami.github.io/blogs/notes/20210224162319-model_evaluations_score/</link><pubDate>Wed, 24 Feb 2021 16:23:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/20210224162319-model_evaluations_score/</guid><description>references To Read: https://towardsdatascience.com/handling-imbalanced-datasets-in-machine-learning-7a0e84220f28 https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin questions ml model evaluation methods
Classfication True Positive (TP) Positive class correctly labeled/predicted False Negative (FN) Positive class incorrectly labeled/predicted False Positive (FP) Negative class incorrectly labeled/predicted True Negative (TN) Negative class correctly labeled/predicted Accuracy It is simply a ratio of correctly predicted observation to the total observations. Accuracy = \(\frac{TP + TN}{TP + TN + FN + FP}\) Precision Precision = \(\frac{True Positive}{True Positive + False Positive}\) From all the postive prediction given by our hypothesis/model how many examples were true positive Recall Recall = \(\frac{True Positive}{True Positive + False Negative}\) From all the postive examples how many examples were correctly classified by our hypothesis/model F1 Score A harmonic mean between recall and precision</description></item><item><title>image segmentation</title><link>https://utsavdarlami.github.io/blogs/notes/20210224161655-image_segmentation/</link><pubDate>Wed, 24 Feb 2021 16:16:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/20210224161655-image_segmentation/</guid><description>references: https://www.youtube.com/watch?v=AZr64OxshLo https://www.youtube.com/watch?v=0FmNxqLFeYo questions : Types of image segmentation Semantic segmentation Instance segmentation Apporaches for segmentation Using Classical Approach Histogram Based Segmentation thresholding Random Walker Segmentation Watershed Segmentation Graph based segmentation &amp;lt;/home/felladog/Downloads/CV_Nptel/W2/DL4CV_Week02_Part05.pdf&amp;gt; https://youtu.be/0HbRnFTOOms?t=680 Using Unsupervised Approach Gaussian Mixture Model Kmeans Clustering Using Deep Learning UNET Image Segmentation Loss/Evaluation Method Problem with other common metrics We need metrics that target pixels in foreground Intersection over union (Jaccard index) sklearn jaccard module</description></item><item><title>machine learning</title><link>https://utsavdarlami.github.io/blogs/notes/20210119123811-machine_learning/</link><pubDate>Sun, 17 Jan 2021 17:28:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/20210119123811-machine_learning/</guid><description>references Reading https://www.cs.cmu.edu/~tom/mlbook.html To Read https://rentruewang.github.io/learning-machine/intro.html https://medium.com/octavian-ai/how-to-get-started-with-machine-learning-on-graphs-7f0795c83763 https://medium.datadriveninvestor.com/3-steps-introduction-to-machine-learning-and-design-of-a-learning-system-bd12b65aa50c https://chrisalbon.com/ Interpretable Machine Learning A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.
Machine learning is a subfield of AI that studies the ability to improve performance based on experience.Some AI systems use machine learning methods to achieve competence, but some do not.</description></item></channel></rss>