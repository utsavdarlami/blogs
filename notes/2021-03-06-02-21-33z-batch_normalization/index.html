<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>Batch Normalization</title><meta name=description content="felladog"><meta name=author content="Utsav Darlami"><link rel="shortcut icon" href=/blogs/img/21-512.png><link rel=stylesheet href=/blogs/css/style.css><link rel=stylesheet href=/blogs/css/syntax.css><link rel=stylesheet href=/blogs/css/toc.css><link rel=stylesheet href=/blogs/katex/katex.min.css><script defer src=/blogs/katex/katex.min.js></script><script defer src=/blogs/katex/contrib/auto-render.min.js onload=renderMathInElement(document.body)></script></head><body><header></header><main><div class=toc><nav id=TableOfContents><ul><li><a href=#how-is-it-done>How is it done</a></li><li><a href=#batchnorm-in-pytorch>BatchNorm in Pytorch</a><ul><li><a href=#implementation-notes>Implementation Notes</a></li></ul></li><li><a href=#batchnorm-during-prediction--inference>BatchNorm during prediction(&ldquo;Inference&rdquo;)</a></li><li><a href=#why-batch-normalization-works>Why batch normalization works?</a></li><li><a href=#summary>Summary</a></li></ul></nav><a href=# class=back-to-top>Back to top</a></div><script src=https://utsavdarlami.github.io/blogs/js/libs/jquery/3.3.1/jquery.slim.min.js></script><script>(function(){var a=$('#TableOfContents'),b;if(a.length>0){b=$(window);function c(){var e=b.scrollTop(),f=$('.body h1, .body h2, .body h3, .body h4, .body h5, .body h6'),c="",d;if(f.each(function(b,a){a=$(a),a.offset().top-10<=e&&(c=a.attr('id'))}),d=a.find('a.current'),d.length==1&&d.eq(0).attr('href')=='#'+c)return!0;d.each(function(b,a){$(a).removeClass('current').siblings('ul').hide()}),a.find('a[href="#'+c+'"]').parentsUntil('#TableOfContents').each(function(b,a){$(a).children('a').addClass('current').siblings('ul').show()})}b.on('scroll',c),$(document).ready(function(){a.find('a').parent('li').find('ul').hide(),c(),document.getElementsByClassName('toc')[0].style.display=''})}})()</script><p align=right><a href=https://utsavdarlami.github.io/>home</a> |
<a href=https://utsavdarlami.github.io/blogs/>blogs</a> |
<a href=https://utsavdarlami.github.io/blogs/notes>notes</a> |
<a href=https://utsavdarlami.github.io/blogs/about>about me</a> |
<a href=https://utsavdarlami.github.io/blogs/tags>tags</a> |
<a href=https://utsavdarlami.github.io/blogs/categories>categories</a> |
<a href=https://utsavdarlami.github.io/blogs/index.xml>feed</a></p><article><h1>Batch Normalization</h1><div style=float:left><time>Created Date : 2021 March 06</time></div><br><div style=float:left><time>Last Modified : 2021 June 03</time></div><hr><div>tags:
<a href=/blogs/tags/batchnorm>BatchNorm</a>
<a href=/blogs/tags/normalization>Normalization</a></div><div style=float:left>categories:
<a href=/blogs/categories/deep-learning>deep learning</a></div><div><hr><ul><li>Acknowledgement :<ul><li><a href=https://d2l.ai/chapter%5Fconvolutional-modern/batch-norm.html>d2l.ai batch normalization</a></li><li><a href="https://www.youtube.com/watch?v=34PDIFvvESc&list=PLTKMiZHVd%5F2KJtIXOW0zFhFfBaJJilH51&index=83">L11.2 How BatchNorm Works - Sebastian Raschka</a></li><li><a href=https://twitter.com/karpathy/status/1013245864570073090>https://twitter.com/karpathy/status/1013245864570073090</a></li></ul></li><li>To read:<ul><li><a href=https://www.reddit.com/r/MachineLearning/comments/nnivo6/d%5Fwhy%5Fis%5Fbatch%5Fnorm%5Fbecoming%5Fso%5Funpopular/>Why is batch norm becoming so unpopular</a></li></ul></li><li>Questions :<ol><li>What is Normalization?<ul><li><a href=/blogs/404.html>Normalization</a></li></ul></li></ol></li></ul><hr><ul><li>A popular and effective technique that constantly accelerates the convergence of deep network.</li><li>Normalizes hidden layer inputs.</li><li>Helps with exploding/vanishing gradient problems.</li><li>Can increase the training stability and convergence rate.</li><li>Can be understood as additional(normalization) layers(with additional parameters).</li></ul><h2 id=how-is-it-done>How is it done</h2><ul><li><p>Batch normalization is applied to individual layers (optionally, to all of them) and works as follows:</p><ul><li>In each training iteration, we first normalize the inputs (of batch normalization) by subtracting their mean and dividing by their standard deviation, where both are estimated based on the statistics of the current minibatch.</li><li>Next, we apply a scale coefficient and a scale offset. It is precisely due to this normalization based on batch statistics that batch normalization derives its name.</li></ul></li><li><p>Formulas :</p><ul><li><p>\(BN(x) = \gamma \odot \frac{x - \hat{\mu}_{B}}{\hat{\sigma}_{B}} + {\beta}\)</p></li><li><p>where,</p><ul><li>\(\hat{\mu}_{B}\) is the sample mean,<ul><li>\(\hat{\mu}_{B} = \frac{1}{|B|} \displaystyle\sum_{x\in{B}} {x}\)</li></ul></li><li>\(\hat{\sigma}_{B}\) is the sample standard deviation of the minibatch B<ul><li>\(\hat{\sigma}_{B} = \frac{1}{|B|} \displaystyle\sum_{x\in{B}} {(x - \hat{\mu}_{B})^2} + \epsilon\)</li><li>\(\epsilon\) is for numerical stability</li></ul></li><li>\(\gamma\) is scale parameter, \(\beta\) is shift parameter, elementwise and have same shape as \(x\), they need to be learned jointly with the other model parameters</li></ul></li></ul><ul><li>\(\gamma\) controls the spread or scales</li><li>\(\beta\) controls the mean,<ul><li>It also makes the bias unit redundant so we can remove the bias from previous layers.</li></ul></li><li>The network can learn \(\gamma\) = \(\hat{\sigma}_{B}\) and \(\beta\) = \(\hat{\mu}_{B}\) undoing the normalization part</li></ul></li></ul><h2 id=batchnorm-in-pytorch>BatchNorm in Pytorch</h2><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>batch_norm</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm2d</span><span class=p>(</span><span class=n>num_features</span><span class=p>,</span> <span class=c1># no of channels</span>
				  <span class=n>eps</span><span class=o>=</span><span class=mf>1e-05</span><span class=p>,</span> <span class=c1># the value used for the running_mean and running_var computation.</span>
				  <span class=n>momentum</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span>
				  <span class=n>affine</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>  <span class=c1># With Learnable Parameters</span>
				  <span class=n>track_running_stats</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</code></pre></div><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=kn>import</span> <span class=nn>torch</span>
<span class=kn>from</span> <span class=nn>torch</span> <span class=kn>import</span> <span class=n>nn</span>

<span class=n>m</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm2d</span><span class=p>(</span><span class=mi>64</span><span class=p>)</span>  <span class=c1># C</span>
<span class=c1># Input : (N, C, H, W)</span>
<span class=n>in_</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>rand</span><span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=mi>64</span><span class=p>,</span> <span class=mi>68</span><span class=p>,</span> <span class=mi>68</span><span class=p>)</span>
<span class=c1># Output : (N, C, H, W) (same shape as input)</span>
<span class=n>out_</span> <span class=o>=</span> <span class=n>m</span><span class=p>(</span><span class=n>in_</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Results : &#34;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=n>out_</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span> <span class=c1># torch.Size([32, 64, 68, 68])</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Mean =  </span><span class=si>{</span><span class=n>out_</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span><span class=o>.</span><span class=n>item</span><span class=p>()</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>  <span class=c1># ~ 0</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;SD  = </span><span class=si>{</span><span class=n>out_</span><span class=o>.</span><span class=n>std</span><span class=p>()</span><span class=o>.</span><span class=n>item</span><span class=p>()</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>   <span class=c1># ~ 1</span>
</code></pre></div><div class=highlight><pre class=chroma><code class=language-text data-lang=text>Results :
torch.Size([32, 64, 68, 68])
Mean =  -4.679170828580936e-09
SD  = 0.999940037727356
</code></pre></div><h3 id=implementation-notes>Implementation Notes</h3><ul><li><p>BatchNorm1d(num_of_units_in_prev_layer) for Linear Layers.</p></li><li><p>BatchNorm2d(prev_out_channels) for Conv Layers.</p></li><li><p>BatchNorm can be used before or <strong>after</strong> the activation layer but it was initial proposed to be used before activation layer. If using Dropout, keep it after the activation layer.</p></li><li><p><em>Use bias=False for your Linear/Conv2d layer when using BatchNorm, or conversely forget to include it for the output layer. - Andrej Karpathy</em>.</p><blockquote class=twitter-tweet><p lang=en dir=ltr>oh: 5) you didn't use bias=False for your Linear/Conv2d layer when using BatchNorm, or conversely forget to include it for the output layer .This one won't make you silently fail, but they are spurious parameters</p>&mdash; Andrej Karpathy (@karpathy) <a href="https://twitter.com/karpathy/status/1013245864570073090?ref_src=twsrc%5Etfw">July 1, 2018</a></blockquote><script async src=https://platform.twitter.com/widgets.js></script></li><li><p>Toggle model.train() and model.eval() for inference.</p></li><li><p>Stable with larger minibatch size > (16, 32, 64)</p></li></ul><h2 id=batchnorm-during-prediction--inference>BatchNorm during prediction(&ldquo;Inference&rdquo;)</h2><ol><li>[ <strong>Preferred Way</strong> ] Using exponentially weighted avg(moving avg) of mean and variance<ul><li>running_mean = momentum * running_mean + (1 - momentum) * sample_mean, where,<ul><li>running_mean is the mean from the previous batch</li><li>sample mean is the mean from the current batch</li><li>momentum is choosed as 0.1</li></ul></li><li>Similarly for variance we can calculate running_variance</li><li>This running mean and variance will be used during the inference for scaling datapoints.</li></ul></li><li>Alternatively can also use global training set mean and variacnce</li></ol><h2 id=why-batch-normalization-works>Why batch normalization works?</h2><ul><li>The first paper on batch norm presented that it reduces the internal covariate shift <em>(It means that the layers distribution changes)</em> but no strong evidence is available.</li><li>One another theory could be that it just provides additional parameters helping in decoupling the layers(make the layers independent). Let&rsquo;s say if one layers screws up the other next layers can be robust if batch norm is applied.</li><li>Paper <a href=https://arxiv.org/abs/1805.11604>How does batch normalization help optimization?</a> shows<ul><li>BatchNorm makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training.</li><li>The BatchNorm enables faster convergence by allowing larger learning rate.</li><li>Good performance seems unrelated to covariate shift reduction.</li></ul></li></ul><h2 id=summary>Summary</h2><ul><li><p>During model training, batch normalization continuously adjusts the intermediate output of the neural network by utilizing the mean and standard deviation of the minibatch, so that the values of the intermediate output in each layer throughout the neural network are more stable.</p></li><li><p>The batch normalization methods for fully-connected layers and convolutional layers are slightly.</p></li><li><p>Like a dropout layer, batch normalization layers have different computation results in training mode and prediction mode.</p></li><li><p>Batch normalization has many beneficial side effects, primarily that of <a href=/blogs/notes/2021-05-26-11-28-39z-regularization/>regularization</a>. On the other hand, the original motivation of reducing internal covariate shift seems not to be a valid explanation.</p></li></ul></div><hr><footer><main><a href=https://github.com/utsavdarlami/blogs/issues/new>Raise issues for Discussion on this post..</a></main></footer><p style=font-size:1.25em;text-align:center><a href=/blogs/notes/20210213135051-recurrent_neural_networks/>&#8678;</a>
<a href=/blogs/notes/2021-03-06-03-56-06z-entropy/>&#8680;</a></p></article><div class=bl-section><h4>Links to this note</h4><div class=backlinks><ul><li><a href=/blogs/notes/2021-05-26-11-28-39z-regularization/>regularization</a></li></ul></div></div></main><footer></footer></body></html>