<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>entropy</title><meta name=description content="felladog"><meta name=author content="Utsav Darlami"><link rel="shortcut icon" href=/blogs/img/21-512.png><link rel=stylesheet href=/blogs/css/style.css><link rel=stylesheet href=/blogs/css/syntax.css><link rel=stylesheet href=/blogs/css/toc.css><link rel=stylesheet href=/blogs/katex/katex.min.css><script defer src=/blogs/katex/katex.min.js></script><script defer src=/blogs/katex/contrib/auto-render.min.js onload=renderMathInElement(document.body)></script></head><body><header></header><main><div class=toc><nav id=TableOfContents><ul><li><a href=#what-is-the-point-of-entropy-in-decision-tree>What is the point of entropy in decision tree?</a></li><li><a href=#application>Application</a></li></ul></nav><a href=# class=back-to-top>Back to top</a></div><script src=https://utsavdarlami.github.io/blogs/js/libs/jquery/3.3.1/jquery.slim.min.js></script><script>(function(){var a=$('#TableOfContents'),b;if(a.length>0){b=$(window);function c(){var e=b.scrollTop(),f=$('.body h1, .body h2, .body h3, .body h4, .body h5, .body h6'),c="",d;if(f.each(function(b,a){a=$(a),a.offset().top-10<=e&&(c=a.attr('id'))}),d=a.find('a.current'),d.length==1&&d.eq(0).attr('href')=='#'+c)return!0;d.each(function(b,a){$(a).removeClass('current').siblings('ul').hide()}),a.find('a[href="#'+c+'"]').parentsUntil('#TableOfContents').each(function(b,a){$(a).children('a').addClass('current').siblings('ul').show()})}b.on('scroll',c),$(document).ready(function(){a.find('a').parent('li').find('ul').hide(),c(),document.getElementsByClassName('toc')[0].style.display=''})}})()</script><p align=right><a href=https://utsavdarlami.github.io/>home</a> |
<a href=https://utsavdarlami.github.io/blogs/>blogs</a> |
<a href=https://utsavdarlami.github.io/blogs/notes>notes</a> |
<a href=https://utsavdarlami.github.io/blogs/about>about me</a> |
<a href=https://utsavdarlami.github.io/blogs/tags>tags</a> |
<a href=https://utsavdarlami.github.io/blogs/categories>categories</a> |
<a href=https://utsavdarlami.github.io/blogs/index.xml>feed</a></p><article><h1>entropy</h1><div style=float:left><time>Created Date : 2021 March 06</time></div><br><div style=float:left><time>Last Modified : 2021 March 06</time></div><hr><div>tags:
<a href=/blogs/tags/entropy>entropy</a></div><div style=float:left>categories:
<a href=/blogs/categories/machine-learning>machine learning</a></div><div><hr><ul><li><p>References :</p><ul><li><a href=https://www.wikiwand.com/en/Entropy%5F(information%5Ftheory)>https://www.wikiwand.com/en/Entropy%5F(information%5Ftheory)</a></li><li><a href=https://scikit-image.org/docs/stable/auto%5Fexamples/filters/plot%5Fentropy.html#sphx-glr-auto-examples-filters-plot-entropy-py>Entropy in image, scikit-image</a></li></ul><p>[3] <a href=https://bricaud.github.io/personal-blog/entropy-in-decision-trees/>https://bricaud.github.io/personal-blog/entropy-in-decision-trees/</a></p></li><li><p>Questions :</p><ul><li>What is the point of entropy in decision tree?</li></ul></li></ul><hr><p><strong>Entropy is a measure of disorder.</strong>
A high entropy is essentially saying that the data is scattered around while a low entropy means that nearly all the data is the same.
The highest entropy one can achieve is 1 and lowest entropy one can achieve is 0.</p><ul><li><p>In information theory, information entropy is the log-base-2 of the number of possible outcomes for a message.</p><ul><li>\(- \log_{2} p\)</li></ul></li><li><p>Based on our dataset we can say</p><ul><li>It is an indicator of how messy your data is.</li><li>Characterizes the (im)purity of an arbitary collection of examples.</li></ul></li><li><p>Given a discrete random variable X, with possible outcomes \(x_{1},&mldr;,x_{n}\) which occur with probability \( {P} (x_{1}),&mldr;, {P} (x_{n}), \) the entropy of X is formally defined as:</p><ul><li>\( H(X) = - \sum_{i=1}^{n} P(x_i) log_2 P(x_i) \) ,</li></ul></li></ul><h2 id=what-is-the-point-of-entropy-in-decision-tree>What is the point of entropy in decision tree?</h2><p><em>At each step, each branching, you want to decrease the entropy, so this quantity is computed before the cut and after the cut. If it decreases, the split is validated and we can proceed to the next step, otherwise, we must try to split with another feature or stop this branch.</em>[3]</p><h2 id=application>Application</h2><ul><li>In <a href=/blogs/404.html>image filtering</a> entropy is used for texture analysis</li><li>In decision tree</li></ul></div><p style=font-size:1.65em;text-align:center><a href=/blogs/notes/2021-03-06-02-21-33z-batch_normalization/>&#8678;</a>
<a href=/blogs/notes/2021-03-31-07-12-59z-text_preprocessing/>&#8680;</a></p><hr><div class=bl-section><h4>Links to this note</h4><div class=backlinks><ul><li><a href=/blogs/notes/2021-06-03-11-01-27z-decision_tree/>decision tree</a></li><li><a href=/blogs/posts/2021-02-28-11-34-36z-blog_using_org_and_hugo/>blog using org and hugo</a></li></ul></div></div><hr><footer><main><h3>Comments</h3><script src=https://utteranc.es/client.js repo=utsavdarlami/blogs issue-term=pathname label=comment theme=github-light crossorigin=anonymous async></script></main></footer></article></main><footer></footer></body></html>