<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>text-preprocessing</title><meta name=description content="felladog"><meta name=author content="Utsav Darlami"><link rel="shortcut icon" href=/blogs/img/21-512.png><link rel=stylesheet href=/blogs/css/style.css><link rel=stylesheet href=/blogs/css/syntax.css><link rel=stylesheet href=/blogs/css/toc.css><link rel=stylesheet href=/blogs/katex/katex.min.css><script defer src=/blogs/katex/katex.min.js></script><script defer src=/blogs/katex/contrib/auto-render.min.js onload=renderMathInElement(document.body)></script></head><body><header></header><main><div class=toc><nav id=TableOfContents><ul><li><a href=#bag-of-words>Bag of Words</a><ul><li><a href=#creating-bag-of-words-using-sklearn>Creating Bag of words using Sklearn</a></li></ul></li><li><a href=#tf-idf>TF-IDF</a><ul><li><a href=#how-to-compute>How to Compute:</a></li><li><a href=#example>Example:</a></li></ul></li></ul></nav><a href=# class=back-to-top>Back to top</a></div><script src=https://utsavdarlami.github.io/blogs/js/libs/jquery/3.3.1/jquery.slim.min.js></script><script>(function(){var a=$('#TableOfContents'),b;if(a.length>0){b=$(window);function c(){var e=b.scrollTop(),f=$('.body h1, .body h2, .body h3, .body h4, .body h5, .body h6'),c="",d;if(f.each(function(b,a){a=$(a),a.offset().top-10<=e&&(c=a.attr('id'))}),d=a.find('a.current'),d.length==1&&d.eq(0).attr('href')=='#'+c)return!0;d.each(function(b,a){$(a).removeClass('current').siblings('ul').hide()}),a.find('a[href="#'+c+'"]').parentsUntil('#TableOfContents').each(function(b,a){$(a).children('a').addClass('current').siblings('ul').show()})}b.on('scroll',c),$(document).ready(function(){a.find('a').parent('li').find('ul').hide(),c(),document.getElementsByClassName('toc')[0].style.display=''})}})()</script><p align=right><a href=https://utsavdarlami.github.io/>home</a> |
<a href=https://utsavdarlami.github.io/blogs/>blogs</a> |
<a href=https://utsavdarlami.github.io/blogs/notes>notes</a> |
<a href=https://utsavdarlami.github.io/blogs/about>about me</a> |
<a href=https://utsavdarlami.github.io/blogs/tags>tags</a> |
<a href=https://utsavdarlami.github.io/blogs/categories>categories</a> |
<a href=https://utsavdarlami.github.io/blogs/index.xml>feed</a></p><article><h1>text-preprocessing</h1><div style=float:left><time>Created Date : 2021 March 31</time></div><br><div style=float:left><time>Last Modified : 2021 June 07</time></div><hr><div>tags:
<a href=/blogs/tags/bag-of-words>bag of words</a>
<a href=/blogs/tags/tf-idf>TF-IDF</a></div><div style=float:left>categories:
<a href=/blogs/categories/nlp>NLP</a>
<a href=/blogs/categories/data-preprocessing>data preprocessing</a></div><div><hr><ul><li>References :<ul><li>[Book] Builiding the ML systems with python</li><li><a href=http://tfidf.com/>http://tfidf.com/</a></li></ul></li><li>Questions :</li></ul><hr><h2 id=bag-of-words>Bag of Words</h2><p>Bag of word approach, it totally ignores the order of words and simply uses word counts as their basis. For each word in the post, its occurrence is counted and noted in a vector. Not surprisingly, this step is also called vectorization. The vector is typically huge as it contains as many elements as words occur in the whole dataset.</p><p>Take, for instance, two example posts with the following word counts:</p><table><thead><tr><th>Word</th><th>Occurrences in Post 1</th><th>Occurrences in post 2</th></tr></thead><tbody><tr><td>disk</td><td>1</td><td>1</td></tr><tr><td>format</td><td>1</td><td>1</td></tr><tr><td>how</td><td>1</td><td>0</td></tr><tr><td>hard</td><td>1</td><td>1</td></tr><tr><td>my</td><td>1</td><td>0</td></tr><tr><td>problems</td><td>0</td><td>1</td></tr><tr><td>to</td><td>1</td><td>0</td></tr></tbody></table><p>The columns occurrences in post 1 and occurrences in post 2 can now be treated as
simple vectors.</p><h3 id=creating-bag-of-words-using-sklearn>Creating Bag of words using Sklearn</h3><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=kn>from</span> <span class=nn>sklearn.feature_extraction.text</span> <span class=kn>import</span> <span class=n>CountVectorizer</span>
<span class=n>vectorizer</span> <span class=o>=</span> <span class=n>CountVectorizer</span><span class=p>(</span><span class=n>min_df</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
<span class=n>content</span> <span class=o>=</span> <span class=p>[</span><span class=s2>&#34;How to format my hard disk&#34;</span><span class=p>,</span> <span class=s2>&#34; Hard disk format problems &#34;</span><span class=p>]</span>
<span class=n>X</span>  <span class=o>=</span> <span class=n>vectorizer</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>content</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=n>vectorizer</span><span class=o>.</span><span class=n>get_feature_names</span><span class=p>())</span>
<span class=nb>print</span><span class=p>(</span><span class=s2>&#34;vectors from post 1 and 2&#34;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=n>X</span><span class=o>.</span><span class=n>toarray</span><span class=p>()</span><span class=o>.</span><span class=n>transpose</span><span class=p>())</span>
</code></pre></div><div class=highlight><pre class=chroma><code class=language-text data-lang=text>[&#39;disk&#39;, &#39;format&#39;, &#39;hard&#39;, &#39;how&#39;, &#39;my&#39;, &#39;problems&#39;, &#39;to&#39;]
vectors from post 1 and 2
[[1 1]
 [1 1]
 [1 1]
 [1 0]
 [1 0]
 [0 1]
 [1 0]]
</code></pre></div><h2 id=tf-idf>TF-IDF</h2><p>Counting term frequencies for every post and in addition discount those that appear in many posts. In other words, we want a high value for a given term in a given value, if that term occurs often in that particular post and very seldom anywhere else.
TF stands for the counting part, while IDF factors in the discounting.</p><h3 id=how-to-compute>How to Compute:</h3><p>Typically, the tf-idf weight is composed by two terms: the first computes the normalized Term Frequency (TF), aka. the number of times a word appears in a document, divided by the total number of words in that document; the second term is the Inverse Document Frequency (IDF), computed as the logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears.</p><h4 id=tf>TF</h4><p>Term Frequency, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization:</p><ul><li>TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).</li></ul><h4 id=idf>IDF</h4><p>Inverse Document Frequency, which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as &ldquo;is&rdquo;, &ldquo;of&rdquo;, and &ldquo;that&rdquo;, may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following:</p><ul><li>IDF(t) = log_e(Total number of documents / Number of documents with term t in it).</li></ul><h3 id=example>Example:</h3><p>Consider a document containing 100 words wherein the word cat appears 3 times. The term frequency (i.e., tf) for cat is then (3 / 100) = 0.03. Now, assume we have 10 million documents and the word cat appears in one thousand of these. Then, the inverse document frequency (i.e., idf) is calculated as log(10,000,000 / 1,000) = 4. Thus, the Tf-idf weight is the product of these quantities: 0.03 * 4 = 0.12.</p></div><p style=font-size:1.65em;text-align:center><a href=/blogs/notes/2021-03-06-03-56-06z-entropy/>&#8678;</a>
<a href=/blogs/notes/2021-05-22-10-42-02z-activation_function/>&#8680;</a></p><hr><hr><footer><main><h3>Comments</h3><script src=https://utteranc.es/client.js repo=utsavdarlami/blogs issue-term=pathname label=comment theme=github-light crossorigin=anonymous async></script></main></footer></article></main><footer></footer></body></html>