<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>neural network</title><meta name=description content="felladog"><meta name=author content="Utsav Darlami"><link rel="shortcut icon" href=/blogs/img/21-512.png><link rel=stylesheet href=/blogs/css/style.css><link rel=stylesheet href=/blogs/css/syntax.css><link rel=stylesheet href=/blogs/css/toc.css><link rel=stylesheet href=/blogs/katex/katex.min.css><script defer src=/blogs/katex/katex.min.js></script><script defer src=/blogs/katex/contrib/auto-render.min.js onload=renderMathInElement(document.body)></script></head><body><header></header><main><div class=toc><nav id=TableOfContents><ul><li><a href=#biological-motivation>Biological Motivation</a></li><li><a href=#representations>Representations</a></li><li><a href=#when-is-neural-network-suitable>When is Neural Network suitable ?</a></li><li><a href=#alvinn-system>ALVINN System</a></li><li><a href=#a-perceptron--20210224173929-perceptron-dot-md--is-a-simple-neural-network>A <a href=HAHAHUGOSHORTCODE-s1-HBHB>perceptron</a> is a simple neural network</a></li><li><a href=#gradient-descent-and-the-delta-rule>Gradient Descent And the Delta Rule</a><ul><li><a href=#for-derivative-of-error--e--w-dot-r-dot-t--i-th--weight>For derivative of error(<em>E</em>) w.r.t \(i_{th}\) weight</a></li><li><a href=#algorithm>Algorithm</a></li></ul></li><li><a href=#about-the-learning-rate-and-gradient-descent>About the Learning Rate and Gradient Descent</a></li><li><a href=#stochastic-approximation-to-gradient-descent>Stochastic Approximation to gradient descent</a></li><li><a href=#gradient-descent-vs-stochastic-gradient-descent>Gradient Descent vs Stochastic Gradient Descent</a></li><li><a href=#delta-rule-vs-perceptron-rule>Delta rule vs Perceptron rule</a></li><li><a href=#multilayer-networks>Multilayer Networks</a><ul><li><a href=#differentiable-threshold-unit>Differentiable Threshold Unit</a></li><li><a href=#sigmoid--2021-05-22-10-42-02z-activation-function-dot-md--unit><a href=HAHAHUGOSHORTCODE-s11-HBHB>Sigmoid</a> unit</a></li></ul></li><li><a href=#backpropagation-algorithm>Backpropagation Algorithm</a><ul><li><a href=#adding-momentum>Adding momentum</a></li><li><a href=#for-convergence>For Convergence</a></li></ul></li><li><a href=#representation-power-of-feedforward-networks>Representation power of feedforward networks</a></li><li><a href=#hypothesis-space-search-and-inductive-bias>Hypothesis space search and Inductive Bias</a></li><li><a href=#other-concepts>Other concepts</a><ul><li><a href=#generalization-overfitting-and-stopping-criterion>Generalization, Overfitting and Stopping Criterion</a></li><li><a href=#weight-decay>Weight decay</a></li><li><a href=#alternative-error>Alternative Error</a></li></ul></li><li><a href=#recurrent-networks--rnn-20210213135051-recurrent-neural-networks-dot-md>Recurrent Networks(<a href=HAHAHUGOSHORTCODE-s20-HBHB>RNN</a>)</a></li><li><a href=#python-implementation>Python implementation</a><ul><li><a href=#trying-w-dot-x-in-vectorized-form>Trying W.x in vectorized form</a></li></ul></li></ul></nav><a href=# class=back-to-top>Back to top</a></div><script src=https://utsavdarlami.github.io/blogs/js/libs/jquery/3.3.1/jquery.slim.min.js></script><script>(function(){var a=$('#TableOfContents'),b;if(a.length>0){b=$(window);function c(){var e=b.scrollTop(),f=$('.body h1, .body h2, .body h3, .body h4, .body h5, .body h6'),c="",d;if(f.each(function(b,a){a=$(a),a.offset().top-10<=e&&(c=a.attr('id'))}),d=a.find('a.current'),d.length==1&&d.eq(0).attr('href')=='#'+c)return!0;d.each(function(b,a){$(a).removeClass('current').siblings('ul').hide()}),a.find('a[href="#'+c+'"]').parentsUntil('#TableOfContents').each(function(b,a){$(a).children('a').addClass('current').siblings('ul').show()})}b.on('scroll',c),$(document).ready(function(){a.find('a').parent('li').find('ul').hide(),c(),document.getElementsByClassName('toc')[0].style.display=''})}})()</script><p align=right><a href=https://utsavdarlami.github.io/>home</a> |
<a href=https://utsavdarlami.github.io/blogs/>blogs</a> |
<a href=https://utsavdarlami.github.io/blogs/notes>notes</a> |
<a href=https://utsavdarlami.github.io/blogs/about>about me</a> |
<a href=https://utsavdarlami.github.io/blogs/tags>tags</a> |
<a href=https://utsavdarlami.github.io/blogs/categories>categories</a> |
<a href=https://utsavdarlami.github.io/blogs/index.xml>feed</a></p><article><h1>neural network</h1><div style=float:left><time>Created Date : 2021 May 26</time></div><br><div style=float:left><time>Last Modified : 2021 June 03</time></div><hr><div>tags:
<a href=/blogs/tags/backpropagation>backpropagation</a>
<a href=/blogs/tags/delta-rule>delta rule</a></div><div style=float:left>categories:
<a href=/blogs/categories/deep-learning>deep learning</a></div><div><hr><ul><li>References :<ul><li>Done Reading:<ul><li><a href=https://github.com/rasbt/stat453-deep-learning-ss20/blob/master/L03-perceptron/L03%5Fperceptron%5Fslides.pdf>Sebastian Raschka Perceptron Lecture Slide</a></li></ul></li><li>Reading:<ul><li>Tom Mitchell Lectures slides of chapter 4</li><li>Tom Mitchell, Machine Learning Chapter 4</li></ul></li><li>To Read:<ul><li><a href=https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/>https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/</a></li><li><a href=https://zzsza.github.io/data/2018/05/13/cs231n-backpropagation-and-neural-networks/>https://zzsza.github.io/data/2018/05/13/cs231n-backpropagation-and-neural-networks/</a></li></ul></li></ul></li><li>Questions :</li></ul><hr><h2 id=biological-motivation>Biological Motivation</h2><ul><li>Mimicing human brain</li></ul><figure><img src=/blogs/ox-hugo/2021-06-02_20-28-10_screenshot.png alt="Figure 1: biological neuron vs aritifical neuron from this post" width=750 height=450><figcaption><p>Figure 1: biological neuron vs aritifical neuron from this <a href=https://zzsza.github.io/data/2018/05/13/cs231n-backpropagation-and-neural-networks/>post</a></p></figcaption></figure><ul><li>properties that match with the human brain<ul><li><em>Many neuron like threshold switching units</em></li><li><em>Many weighted interconnections among units</em></li><li><em>Highly parallel, distributed process</em></li><li><em>Emphasis on tuning weights automatically</em></li></ul></li></ul><h2 id=representations>Representations</h2><ul><li>The ANNs can graphs be with many types of structures:<ul><li>acyclic or cyclic</li><li>directed or undirected</li></ul></li><li>The backprop algorithm assumes ANN to have structure that corresponds to a directed graph, and possibly containing cyclees.</li><li>Practical applications involves acyclic feed forward network structure for ANN</li></ul><h2 id=when-is-neural-network-suitable>When is Neural Network suitable ?</h2><ol><li>Instances are represented by many attribute value pairs<ul><li>has high dimensions</li></ul></li><li>The target function output may be<ul><li>discrete-valued,</li><li>real-valued</li><li>or a vector of several real or discrete valued attributes</li></ul></li><li>The training examples may contain noises</li><li>Long training times are acceptable</li><li>Fast evaluation of the learned target function may be required.</li><li>The ability to understand the learned target function is not important</li></ol><h2 id=alvinn-system>ALVINN System</h2><ul><li>Autonomous Land Vehicle In a Neural Network</li><li>30 * 32 Sensor input retina as input vector</li><li>outputs vector of 30 attributes [ gives directions]</li></ul><h2 id=a-perceptron--20210224173929-perceptron-dot-md--is-a-simple-neural-network>A <a href=/blogs/notes/20210224173929-perceptron/>perceptron</a> is a simple neural network</h2><figure><img src=/blogs/ox-hugo/2021-06-02_21-27-38_screenshot.png alt="Figure 2: from Sebastian Raschka lectures" width=750 height=450><figcaption><p>Figure 2: from Sebastian Raschka lectures</p></figcaption></figure><h2 id=gradient-descent-and-the-delta-rule>Gradient Descent And the Delta Rule</h2><ul><li>To overcome the problem of failing to coverge on a non linearly separable data a training rule <em>delta rule</em> was designed.</li><li><strong>delta rule</strong> uses <a href=/blogs/notes/2021-05-30-08-53-22z-gradient/>gradient</a> descent to search the hypothesis space \(H\) to find the best weights for our training examples.<ul><li>Space \(H\) of candidate hypotheses is<ul><li>\(H = \{ \overrightarrow{w} | \overrightarrow{w} \in \Re^{n+1} \}\)</li></ul></li></ul></li></ul><figure><img src=/blogs/ox-hugo/2021-06-02_22-32-02_screenshot.png alt="Figure 3: Error as function of w, from Tom Mitchell Lectures" width=700 height=300><figcaption><p>Figure 3: Error as function of w, from Tom Mitchell Lectures</p></figcaption></figure><ul><li>\(d\) is a training example at an instance</li><li>\(t_{d}\) is target output</li><li>\(o_{d}\) is output from linear unit, unthresholded<ul><li>for <a href=/blogs/notes/20210224173929-perceptron/>perceptron</a> \(o_{d} = sgn(\overrightarrow{w}.\overrightarrow{x})\), where,<ul><li>\( sgn(y) = \begin{cases} 1 & \text{if y > 0} \\ -1 & \text{otherwise} \end{cases} \)</li></ul></li></ul></li><li>\(E\) is the Error as a function of \(\overrightarrow{w}\)</li></ul><figure><img src=/blogs/ox-hugo/2021-06-03_09-27-28_screenshot.png alt="Figure 4: Gradient Descent Rule, from Tom Mitchell Lectures" width=450 height=400><figcaption><p>Figure 4: Gradient Descent Rule, from Tom Mitchell Lectures</p></figcaption></figure><ul><li>\( \nabla E(\overrightarrow{w}) \)<ul><li>is called gradient of E with respect to \(\overrightarrow{w}\)</li><li>is a vector</li><li>As a vector in a weight space, the gradient specifies the direction that produces the steepes increase in E</li><li>So the negative of this vector gives the direction of steepest decrease, \( (-\nabla E(\overrightarrow{w})) \)</li><li>We look for steepest decrease because we want to move the weight vector in the direction that decreases E.</li></ul></li></ul><h3 id=for-derivative-of-error--e--w-dot-r-dot-t--i-th--weight>For derivative of error(<em>E</em>) w.r.t \(i_{th}\) weight</h3><figure><img src=/blogs/ox-hugo/2021-06-03_09-44-31_screenshot.png alt="Figure 5: Gradient Descent, E w.r.t wi, from Tom Mitchell Lectures" width=400 height=300><figcaption><p>Figure 5: Gradient Descent, E w.r.t wi, from Tom Mitchell Lectures</p></figcaption></figure><ul><li>\( \Delta{w_{i}} = \eta \sum_{d \in D} (t_{d} - o_{d}) x_{id} \)</li><li>Here <em>i</em> represents the <em>ith</em> feature/component</li></ul><h3 id=algorithm>Algorithm</h3><p></p><figure><img src=/blogs/ox-hugo/gd_algo.png alt="Figure 6: Gradient Descent Algo , from Tom Mitchell Book" width=600 height=350><figcaption><p>Figure 6: Gradient Descent Algo , from Tom Mitchell Book</p></figcaption></figure><ul><li>Computes weight updates after summing over all the training examples in D</li></ul><h2 id=about-the-learning-rate-and-gradient-descent>About the Learning Rate and Gradient Descent</h2><figure><img src=/blogs/ox-hugo/2021-06-03_09-17-13_screenshot.png alt="Figure 7: from Sebastian Raschka letcures" width=450 height=350><figcaption><p>Figure 7: from Sebastian Raschka letcures</p></figcaption></figure><h2 id=stochastic-approximation-to-gradient-descent>Stochastic Approximation to gradient descent</h2><ul><li><p>The key practical difficulties in applying gradient descent are</p><ul><li>converging to a local minimum can sometimes be quite slow (i.e.,it can require many thousands of gradient descent steps), and</li><li>if there are multiple local minima in the error surface, then there is no guarantee that the procedure will find the global minimum.</li></ul></li><li><p>For such problem <strong>stochastic</strong> (random) gradient descent <strong>(incremental gradient descent)</strong> was introduced.</p></li><li><p>The idea behind stochastic gradient descent is to approximate this gradient descent search by updating weights incrementally, following the calculation of the error for each individual example.</p></li><li><p>To modify the gradient descent algorithm shown in <em>Figure <a href=#org265958d>6</a></em> to implement this stochastic approximation, Equation (T4.2) is simply deleted and Equation (T4.1) replaced by,</p><ul><li>\( w_{i} \leftarrow w_{i} + \eta (t - o) x_{i} \)</li></ul></li><li><p>here, \( \Delta w_{i} = \eta (t - o) x_{i} \)</p><ul><li>This is Delta rule, also Known as<ul><li>Adaline Rule</li><li>LMS (least mean square)</li><li>Widrow-Hoff (inventors of the rule)</li></ul></li></ul></li></ul><h2 id=gradient-descent-vs-stochastic-gradient-descent>Gradient Descent vs Stochastic Gradient Descent</h2><figure><img src=/blogs/ox-hugo/2021-06-03_10-57-13_screenshot.png alt="Figure 8: GD vs SGD, from Tom Mitchell Lectures" width=450 height=420><figcaption><p>Figure 8: GD vs SGD, from Tom Mitchell Lectures</p></figcaption></figure><ul><li>In standard gradient descent, the error is summed over all examples before updating weights, whereas in stochastic gradient descent weights are updated upon examining each training example.</li><li>Summing over multiple examples in standard gradient descent requires more computation per weight update step. On the other hand, because it uses the true gradient, standard gradient descent is often used with a larger step size per weight update than stochastic gradient descent.</li><li>In cases where there are multiple local minima with respect to \(E(\overrightarrow{w})\), stochastic gradient descent can sometimes avoid falling into these local minima because it uses the various \(\Delta E_{d}(\overrightarrow{w})\) rather than \(\Delta E(\overrightarrow{w})\) to guide its search.</li></ul><h2 id=delta-rule-vs-perceptron-rule>Delta rule vs Perceptron rule</h2><ul><li><em>The difference between these two training rules is reflected in different convergence properties.</em><ul><li><em>The <strong>perceptron training rule</strong> converges after a finite number of iterations to a hypothesis that perfectly classifies the training data, provided the training examples are linearly separable.</em></li><li><em>The <strong>delta rule</strong> converges only asymptotically toward the minimum error hypothesis, possibly requiring unbounded time, but converges regardless of whether the training data are linearly separable.</em></li></ul></li><li>delta rule can be easily extended to multilayer neural network</li></ul><h2 id=multilayer-networks>Multilayer Networks</h2><ul><li>Capable of expressing a rich variety of non linear decision</li></ul><h3 id=differentiable-threshold-unit>Differentiable Threshold Unit</h3><ul><li>We need to introduced units which can help represents non linearity</li><li>Perceptron unit can be used but is discontinuous threshold makes it undifferentiable.</li></ul><h3 id=sigmoid--2021-05-22-10-42-02z-activation-function-dot-md--unit><a href=/blogs/notes/2021-05-22-10-42-02z-activation_function/>Sigmoid</a> unit</h3><ul><li>output is a non linear function of its inputs and is differentiable threshold function</li><li><a href=/blogs/notes/2021-05-22-10-42-02z-activation_function/>Tanh</a> is also used in place of the sigmoid</li><li>error gradient with sigmoid</li></ul><figure><img src=/blogs/ox-hugo/2021-06-03_15-58-13_screenshot.png alt="Figure 9: error gradient with sigmoid unit, from Tom Mitchell Lectures" width=360 height=340><figcaption><p>Figure 9: error gradient with sigmoid unit, from Tom Mitchell Lectures</p></figcaption></figure><h2 id=backpropagation-algorithm>Backpropagation Algorithm</h2><figure><img src=/blogs/ox-hugo/2021-06-03_16-09-25_screenshot.png alt="Figure 10: error gradient with sigmoid unit, from Tom Mitchell Lectures" width=360 height=340><figcaption><p>Figure 10: error gradient with sigmoid unit, from Tom Mitchell Lectures</p></figcaption></figure><ul><li><p>\(\sum_{k \in outputs} w_{hk} \delta_{k}\) = error term for hidden unit h</p><ul><li>since training examples provide target values \(t_{k}\) only for network outputs, no target values are directly available to indicate the error of hidden units' values.</li><li>\(w_{hk}\) is weight form hidden unit \(h\) to output unit \(k\)</li><li>This weight characterizes the degree to which hidden unit h is &ldquo;responsible for&rdquo; the error in output unit k.</li></ul></li><li><p>Easily generalizes to arbitary directed graphs</p></li><li><p>It is peformed only during training</p></li></ul><h3 id=adding-momentum>Adding momentum</h3><ul><li><p>making the weight update on the nth iteration depend partially on the update that occurred during the (n - 1)th iteration, as</p><ul><li>\(\Delta w_{ji}(n) = \eta \delta_{j} x_{ji} + \alpha \Delta wji(n - 1)\)</li></ul><p>Here \(\Delta w_{ji}(n)\) is the weight update performed during the nth iteration through the main loop of the algorithm, and \(0 &lt; \alpha &lt; 1\) is a constant called the momentum.</p><ul><li>\( \eta \delta_{j} x_{ji} \sim \eta \frac{\partial E}{\partial w_{i}} \)</li></ul></li><li><p>Momentum can sometimes carry the gradient descent procedure through narrow local minima (though in principle it can also carry it through narrow global minima into other local minima!).</p></li></ul><h3 id=for-convergence>For Convergence</h3><ul><li>Train multiple nets with different initial weights</li><li>Add momentum</li><li>Stochastic Gradient descent</li></ul><h2 id=representation-power-of-feedforward-networks>Representation power of feedforward networks</h2><ul><li>Boolean functions</li><li>Continuous functions</li><li>Arbitrary Functions</li></ul><h2 id=hypothesis-space-search-and-inductive-bias>Hypothesis space search and Inductive Bias</h2><ul><li>Hypothesis space is continuous, in contrast to the hypothesis spaces of <a href=/blogs/notes/2021-06-03-11-01-27z-decision_tree/>decision tree</a> learning and other methods based on discrete representations.</li><li>Hypothesis space is the n-dimensional Euclidean space of the n network weights. i.e For Backpropagation, every possible assignment of network weights represents a syntactically distinct hypothesis that in principle can be considered by the learner.</li><li><a href=/blogs/notes/2021-06-03-11-07-01z-inductive_bias/>Inductive Bias</a><ul><li>One can roughly characterize it as smooth interpolation between data points.</li></ul></li></ul><h2 id=other-concepts>Other concepts</h2><h3 id=generalization-overfitting-and-stopping-criterion>Generalization, Overfitting and Stopping Criterion</h3><figure><img src=/blogs/ox-hugo/2021-06-03_17-47-59_screenshot.png alt="Figure 11: overfiiting , from Tom Mitchell Lectures" width=400 height=550><figcaption><p>Figure 11: overfiiting , from Tom Mitchell Lectures</p></figcaption></figure><ul><li>In first plot, generalization accuracy measured over the validation examples first decreases, then increases, even as the error over the training examples continues to decrease.<ul><li>this occurs because the weights are being tuned to fit oddity of the training examples that are not representative of the general distribution of examples.</li></ul></li><li>Notice in the second plot, one must be careful to not stop training too soon when the validation set error begins to increase.</li><li>Overfitting tend to occur during later iterations, but not during earlier iterations<ul><li>As training proceeds, some weights begin to grow in order to reduce the error over the training data, and the complexity of the learned decision surface increases.</li></ul></li><li>How many weight-tuning iterations should the algorithm perform?<ul><li>it should use the number of iterations that produces the lowest error over the <strong>validation set</strong>.</li><li>We can try <a href=/blogs/notes/2021-05-26-11-28-39z-regularization/>Early Stopping</a></li><li>K-fold cross-validation approach is sometimes used, in which cross validation is performed K different times, each time using a different partitioning of the data into training(k-1 part of data) and validation sets(1 part of data) , and the results are then averaged.</li></ul></li></ul><h3 id=weight-decay>Weight decay</h3><ul><li>One of the solutions to solve overfitting</li><li>Decrease each weight by some small factor during each iteration.</li><li>This is equivalent to modifying the definition of E to include a penalty term corresponding to the total magnitude of the network weights.</li></ul><h3 id=alternative-error>Alternative Error</h3><h4 id=penalty-term-for-weight-magnitude>Penalty term for weight magnitude</h4><ul><li>To solve overfitting as mentioned in <a href=#weight-decay>Weight decay</a></li><li>Add a term to E that increases with the magnitude of the weight vector.<ul><li>This causes the gradient descent search to seek weight vectors with small magnitudes, thereby reducing the risk of overfitting.</li></ul></li><li>\( E(\overrightarrow{w}) = \frac{1}{2} \sum_{d \in D} \sum_{k \in outputs} (t_{kd} - o_{kd})^2 + \gamma \sum_{i,j} w_{ji} ^ 2 \)</li><li>\( \sum_{i,j} w_{ji} ^2 \) is the <a href=/blogs/notes/2021-05-26-11-28-39z-regularization/>regularization</a> term</li><li>\(\gamma\) controls the importance of the regularization term</li></ul><h4 id=train-on-target-slopes-as-well-as-values>Train on target slopes as well as values</h4><ul><li>Adding a term for errors in the slope, or derivative of the target function.</li><li>\( E(\overrightarrow{w}) = \frac{1}{2} \sum_{d \in D} \sum_{k \in outputs} [ (t_{kd} - o_{kd})^2 +
\mu \sum_{j \in inputs} (\frac{\partial t_{kd}}{\partial x_{d}^j} - \frac{\partial o_{kd}}{\partial x_{d}^j} ) ^ 2 ] \)</li><li>\(\frac{\partial t_{kd}}{\partial x_{d}^j}\) describes how the target output value should vary with a change in the input.</li><li>\(\frac{\partial o_{kd}}{\partial x_{d}^j}\) describes how the observed value should vary with a change in the input.</li><li>The constant \(\mu\) determines the relative weight placed on fitting the training values versus thetraining derivatives.</li></ul><h4 id=minimizing-the-cross-entropy-of-the-network-with-respect-to-the-target-values-dot>Minimizing the cross entropy of the network with respect to the target values.</h4><ul><li>\( - \sum_{d \in D} t_{d} log(o_{d}) + (1 - t_{d})(1 - log(o_{d})) \)</li></ul><h2 id=recurrent-networks--rnn-20210213135051-recurrent-neural-networks-dot-md>Recurrent Networks(<a href=/blogs/notes/20210213135051-recurrent_neural_networks/>RNN</a>)</h2><h2 id=python-implementation>Python implementation</h2><h3 id=trying-w-dot-x-in-vectorized-form>Trying W.x in vectorized form</h3><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=kn>import</span> <span class=nn>numpy</span> <span class=kn>as</span> <span class=nn>np</span>

<span class=n>x</span> <span class=o>=</span> <span class=p>[[</span><span class=mf>1.</span><span class=p>,</span><span class=mf>2.</span><span class=p>,</span><span class=mf>3.</span><span class=p>]]</span>
<span class=n>x_np</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
<span class=k>print</span><span class=p>(</span><span class=n>x_np</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>

<span class=n>w</span> <span class=o>=</span> <span class=p>[[</span><span class=mf>0.1</span><span class=p>,</span><span class=mf>0.2</span><span class=p>,</span><span class=mf>0.3</span><span class=p>]]</span>
<span class=n>w_np</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>w</span><span class=p>)</span>

<span class=k>print</span><span class=p>(</span><span class=n>w_np</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
<span class=c1># Passing SIngle Point</span>
<span class=n>z</span> <span class=o>=</span> <span class=n>x_np</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>w_np</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>
<span class=k>print</span><span class=p>(</span><span class=n>z</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
<span class=k>print</span><span class=p>(</span><span class=n>f</span><span class=s2>&#34;Single datapoint, z = {z.sum()}&#34;</span><span class=p>)</span>
<span class=c1># Passing Batch</span>
<span class=n>z</span> <span class=o>=</span> <span class=n>w_np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>x_np</span><span class=o>.</span><span class=n>T</span><span class=p>)</span>
<span class=k>print</span><span class=p>(</span><span class=n>z</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
<span class=k>print</span><span class=p>(</span><span class=n>f</span><span class=s2>&#34;Batch format, z = {z.sum()}&#34;</span><span class=p>)</span>
</code></pre></div><div class=highlight><pre class=chroma><code class=language-text data-lang=text>(1, 3)
(1, 3)
()
Single datapoint, z = 1.4
(1, 1)
Batch format, z = 1.4
</code></pre></div></div><hr><footer><main><a href=https://github.com/utsavdarlami/blogs/issues/new>Raise issues for Discussion on this post..</a></main></footer><p style=font-size:1.25em;text-align:center><a href=/blogs/notes/20210224173929-perceptron/>&#8678;</a>
<a href=/blogs/notes/2021-05-30-08-53-22z-gradient/>&#8680;</a></p></article><div class=bl-section><h4>Links to this note</h4><div class=backlinks><ul><li><a href=/blogs/notes/20210224173929-perceptron/>perceptron</a></li></ul></div></div></main><footer></footer></body></html>