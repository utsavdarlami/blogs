<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>gradient</title><meta name=description content="felladog"><meta name=author content="Utsav Darlami"><link rel="shortcut icon" href=/blogs/img/21-512.png><link rel=stylesheet href=/blogs/css/style.css><link rel=stylesheet href=/blogs/css/syntax.css><link rel=stylesheet href=/blogs/css/toc.css><link rel=stylesheet href=/blogs/katex/katex.min.css><script defer src=/blogs/katex/katex.min.js></script>
<script defer src=/blogs/katex/contrib/auto-render.min.js onload=renderMathInElement(document.body)></script></head><body><header></header><main><div class=toc><nav id=TableOfContents><ul><li><ul><li><a href=#example>Example</a></li></ul></li><li><a href=#gradients-and-the-multivariable-chain-rule>Gradients and The Multivariable Chain Rule</a></li><li><a href=#the-jacobian-matrix>The Jacobian Matrix</a></li></ul></nav><a href=# class=back-to-top>Back to top</a></div><script src=https://utsavdarlami.github.io/blogs/js/libs/jquery/3.3.1/jquery.slim.min.js></script>
<script>(function(){var t,e=$("#TableOfContents");if(e.length>0){t=$(window);function n(){var n,o=t.scrollTop(),i=$(".body h1, .body h2, .body h3, .body h4, .body h5, .body h6"),s="";if(i.each(function(e,t){t=$(t),t.offset().top-10<=o&&(s=t.attr("id"))}),n=e.find("a.current"),n.length==1&&n.eq(0).attr("href")=="#"+s)return!0;n.each(function(e,t){$(t).removeClass("current").siblings("ul").hide()}),e.find('a[href="#'+s+'"]').parentsUntil("#TableOfContents").each(function(e,t){$(t).children("a").addClass("current").siblings("ul").show()})}t.on("scroll",n),$(document).ready(function(){e.find("a").parent("li").find("ul").hide(),n(),document.getElementsByClassName("toc")[0].style.display=""})}})()</script><p align=right><a href=https://utsavdarlami.github.io/blogs/about>Me</a> |
<a href=https://utsavdarlami.github.io/blogs/>blogs</a> |
<a href=https://utsavdarlami.github.io/blogs/notes>notes</a> |
<a href=https://utsavdarlami.github.io/blogs/tags>tags</a> |
<a href=https://utsavdarlami.github.io/blogs/categories>categories</a> |
<a href=https://utsavdarlami.github.io/blogs/index.xml>feed</a> |
<a href=https://utsavdarlami.github.io/>home</a> |</p><article><h1>gradient</h1><div style=float:left><time>Created Date : 2021 May 30</time></div><br><div style=float:left><time>Last Modified : 2021 July 13</time></div><hr><div>tags:
<a href=/tags/gradient>gradient</a></div><div style=float:left>categories:
<a href=/categories/calculus>calculus</a></div><div><hr><ul><li>References :<ul><li>Done Reading:<ul><li><a href=https://github.com/rasbt/stat453-deep-learning-ss20/blob/master/L05-grad-descent/L05%5Fgrad-descent%5F%5Fslides.pdf>Sebastian Raschka Gradient Descent Lecture Slide</a></li></ul></li><li>To Read:<ul><li><a href=https://www.wikiwand.com/en/Gradient>https://www.wikiwand.com/en/Gradient</a></li></ul></li></ul></li><li>Questions :</li></ul><hr><p>In vector calculus, the gradient is a multi-variable generalization of the derivative.
Whereas the ordinary derivative of a function of a single variable is a scalar-valued function, the gradient of a function of several variables is a vector-valued function</p><p>Simply, derivative of multivariable function</p><p>The gradient (or gradient vector field) of a scalar function f(x1, x2, x3, …, xn) is denoted ∇f or ∇→f where ∇ (nabla) denotes the vector differential operator, del.</p><p>\( \nabla f(p) = { \begin {bmatrix} {\frac {\partial f}{\partial x_{1}}}(p) \\ \vdots \\ {\frac {\partial f}{\partial x_{n}}}(p) \end{bmatrix}} \)</p><h3 id=example>Example</h3><p>\(f(x, y) = x^{2}y + y\),</p><p>\( \nabla f(x, y)
= {\begin{bmatrix}{\frac {\partial f}{\partial x}}(x^{2}y + y)\\ \\ {\frac {\partial f}{\partial y}}(x^{2}y + y)\end{bmatrix}}
= {\begin{bmatrix}(2xy) \\ \\ (x^{2} + 1)\end{bmatrix}} \)</p><h2 id=gradients-and-the-multivariable-chain-rule>Gradients and The Multivariable Chain Rule</h2><figure><img src=/blogs/ox-hugo/2021-06-03_08-54-12_screenshot.png alt="Figure 1: A example" width=400 height=350><figcaption><p>Figure 1: A example</p></figcaption></figure><figure><img src=/blogs/ox-hugo/2021-06-03_08-56-26_screenshot.png alt="Figure 2: A example" width=700 height=300><figcaption><p>Figure 2: A example</p></figcaption></figure><ul><li>Result<ul><li>\( \frac{d}{dx}[f(g(x), h(x))]\ = [2gh.3] + [(g^2 + 1).2x] = 2xg^2 + 6gh + 2x \)</li></ul></li></ul><figure><img src=/blogs/ox-hugo/2021-06-03_09-10-34_screenshot.png alt="Figure 3: In vector form" width=650 height=500><figcaption><p>Figure 3: In vector form</p></figcaption></figure><h2 id=the-jacobian-matrix>The Jacobian Matrix</h2><figure><img src=/blogs/ox-hugo/2021-06-03_09-13-45_screenshot.png alt="Figure 4: Jacobian Matrix" width=500 height=400><figcaption><p>Figure 4: Jacobian Matrix</p></figcaption></figure></div><p style=font-size:1.65em;text-align:center><a href=/blogs/notes/2021-05-28--12-02-59z--image_convolution/>&#8678;</a>
<a href=/blogs/notes/2021-05-31--03-54-38z--edge_filters/>&#8680;</a></p><hr><div class=bl-section><h4>Links to this note</h4><div class=backlinks><ul><li><a href=/blogs/notes/2021-05-26--11-31-31z--neural_network/>neural network</a></li></ul></div></div><hr><footer><main><h3>Comments</h3><script src=https://utteranc.es/client.js repo=utsavdarlami/blogs issue-term=pathname label=comment theme=github-light crossorigin=anonymous async></script></main></footer></article></main><footer></footer></body></html>