<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>gradient</title><meta name=description content="felladog"><meta name=author content="Utsav Darlami"><link rel="shortcut icon" href=/blogs/img/21-512.png><link rel=stylesheet href=/blogs/css/style.css><link rel=stylesheet href=/blogs/css/syntax.css><link rel=stylesheet href=/blogs/css/toc.css><link rel=stylesheet href=/blogs/katex/katex.min.css><script defer src=/blogs/katex/katex.min.js></script><script defer src=/blogs/katex/contrib/auto-render.min.js onload=renderMathInElement(document.body)></script></head><body><header></header><main><div class=toc><nav id=TableOfContents><ul><li><ul><li><a href=#example>Example</a></li></ul></li><li><a href=#gradients-and-the-multivariable-chain-rule>Gradients and The Multivariable Chain Rule</a></li><li><a href=#the-jacobian-matrix>The Jacobian Matrix</a></li></ul></nav><a href=# class=back-to-top>Back to top</a></div><script src=https://utsavdarlami.github.io/blogs/js/libs/jquery/3.3.1/jquery.slim.min.js></script><script>(function(){var a=$('#TableOfContents'),b;if(a.length>0){b=$(window);function c(){var e=b.scrollTop(),f=$('.body h1, .body h2, .body h3, .body h4, .body h5, .body h6'),c="",d;if(f.each(function(b,a){a=$(a),a.offset().top-10<=e&&(c=a.attr('id'))}),d=a.find('a.current'),d.length==1&&d.eq(0).attr('href')=='#'+c)return!0;d.each(function(b,a){$(a).removeClass('current').siblings('ul').hide()}),a.find('a[href="#'+c+'"]').parentsUntil('#TableOfContents').each(function(b,a){$(a).children('a').addClass('current').siblings('ul').show()})}b.on('scroll',c),$(document).ready(function(){a.find('a').parent('li').find('ul').hide(),c(),document.getElementsByClassName('toc')[0].style.display=''})}})()</script><p align=right><a href=https://utsavdarlami.github.io/>home</a> |
<a href=https://utsavdarlami.github.io/blogs/>blogs</a> |
<a href=https://utsavdarlami.github.io/blogs/notes>notes</a> |
<a href=https://utsavdarlami.github.io/blogs/about>about me</a> |
<a href=https://utsavdarlami.github.io/blogs/tags>tags</a> |
<a href=https://utsavdarlami.github.io/blogs/categories>categories</a> |
<a href=https://utsavdarlami.github.io/blogs/index.xml>feed</a></p><article><h1>gradient</h1><div style=float:left><time>Created Date : 2021 May 30</time></div><br><div style=float:left><time>Last Modified : 2021 June 03</time></div><hr><div>tags:
<a href=/blogs/tags/gradient>gradient</a></div><div style=float:left>categories:
<a href=/blogs/categories/calculus>calculus</a></div><div><hr><ul><li>References :<ul><li>Done Reading:<ul><li><a href=https://github.com/rasbt/stat453-deep-learning-ss20/blob/master/L05-grad-descent/L05%5Fgrad-descent%5F%5Fslides.pdf>Sebastian Raschka Gradient Descent Lecture Slide</a></li></ul></li><li>To Read:<ul><li><a href=https://www.wikiwand.com/en/Gradient>https://www.wikiwand.com/en/Gradient</a></li></ul></li></ul></li><li>Questions :</li></ul><hr><p>In vector calculus, the gradient is a multi-variable generalization of the derivative.
Whereas the ordinary derivative of a function of a single variable is a scalar-valued function, the gradient of a function of several variables is a vector-valued function</p><p>Simply, derivative of multivariable function</p><p>The gradient (or gradient vector field) of a scalar function f(x1, x2, x3, …, xn) is denoted ∇f or ∇→f where ∇ (nabla) denotes the vector differential operator, del.</p><p>\( \nabla f(p) = { \begin {bmatrix} {\frac {\partial f}{\partial x_{1}}}(p) \\ \vdots \\ {\frac {\partial f}{\partial x_{n}}}(p) \end{bmatrix}} \)</p><h3 id=example>Example</h3><p>\(f(x, y) = x^{2}y + y\),</p><p>\( \nabla f(x, y)
= {\begin{bmatrix}{\frac {\partial f}{\partial x}}(x^{2}y + y)\\ \\ {\frac {\partial f}{\partial y}}(x^{2}y + y)\end{bmatrix}}
= {\begin{bmatrix}(2xy) \\ \\ (x^{2} + 1)\end{bmatrix}} \)</p><h2 id=gradients-and-the-multivariable-chain-rule>Gradients and The Multivariable Chain Rule</h2><figure><img src=/blogs/ox-hugo/2021-06-03_08-54-12_screenshot.png alt="Figure 1: A example" width=400 height=350><figcaption><p>Figure 1: A example</p></figcaption></figure><figure><img src=/blogs/ox-hugo/2021-06-03_08-56-26_screenshot.png alt="Figure 2: A example" width=700 height=300><figcaption><p>Figure 2: A example</p></figcaption></figure><ul><li>Result<ul><li>\( \frac{d}{dx}[f(g(x), h(x))]\ = [2gh.3] + [(g^2 + 1).2x] = 2xg^2 + 6gh + 2x \)</li></ul></li></ul><figure><img src=/blogs/ox-hugo/2021-06-03_09-10-34_screenshot.png alt="Figure 3: In vector form" width=650 height=500><figcaption><p>Figure 3: In vector form</p></figcaption></figure><h2 id=the-jacobian-matrix>The Jacobian Matrix</h2><figure><img src=/blogs/ox-hugo/2021-06-03_09-13-45_screenshot.png alt="Figure 4: Jacobian Matrix" width=500 height=400><figcaption><p>Figure 4: Jacobian Matrix</p></figcaption></figure></div><hr><footer><main><a href=https://github.com/utsavdarlami/blogs/issues/new>Raise issues for Discussion on this post..</a></main></footer><p style=font-size:1.25em;text-align:center><a href=/blogs/notes/2021-05-26-11-31-31z-neural_network/>&#8678;</a>
<a href=/blogs/notes/2021-06-03-11-01-27z-decision_tree/>&#8680;</a></p></article><div class=bl-section><h4>Links to this note</h4><div class=backlinks><ul><li><a href=/blogs/notes/2021-05-26-11-31-31z-neural_network/>neural network</a></li></ul></div></div></main><footer></footer></body></html>