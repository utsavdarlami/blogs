<!doctype html><html lang=en><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><title>perceptron</title>
<meta name=description content="felladog"><meta name=author content="Utsav Darlami"><link rel="shortcut icon" href=/blogs/img/21-512.png><link rel=stylesheet href=/blogs/css/style.css><link rel=stylesheet href=/blogs/css/syntax.css><link rel=stylesheet href=/blogs/css/toc.css><link rel=stylesheet href=/blogs/katex/katex.min.css><script defer src=/blogs/katex/katex.min.js></script><script defer src=/blogs/katex/contrib/auto-render.min.js onload=renderMathInElement(document.body)></script></head><body><header></header><main><div class=toc><nav id=TableOfContents><ul><li><a href=#representations>Representations</a></li><li><a href=#learning-rule>Learning Rule</a></li><li><a href=#geometric-intuition>Geometric Intuition</a></li></ul></nav><a href=# class=back-to-top>Back to top</a></div><script src=https://utsavdarlami.github.io/blogs/js/libs/jquery/3.3.1/jquery.slim.min.js></script><script>(function(){var t,e=$("#TableOfContents");if(e.length>0){t=$(window);function n(){var n,o=t.scrollTop(),i=$(".body h1, .body h2, .body h3, .body h4, .body h5, .body h6"),s="";if(i.each(function(e,t){t=$(t),t.offset().top-10<=o&&(s=t.attr("id"))}),n=e.find("a.current"),n.length==1&&n.eq(0).attr("href")=="#"+s)return!0;n.each(function(e,t){$(t).removeClass("current").siblings("ul").hide()}),e.find('a[href="#'+s+'"]').parentsUntil("#TableOfContents").each(function(e,t){$(t).children("a").addClass("current").siblings("ul").show()})}t.on("scroll",n),$(document).ready(function(){e.find("a").parent("li").find("ul").hide(),n(),document.getElementsByClassName("toc")[0].style.display=""})}})()</script><p align=right><a href=https://utsavdarlami.github.io/blogs/about>me</a> |
<a href=https://utsavdarlami.github.io/blogs/>blogs</a> |
<a href=https://utsavdarlami.github.io/blogs/notes>notes</a> |
<a href=https://utsavdarlami.github.io/blogs/tags>tags</a> |
<a href=https://utsavdarlami.github.io/blogs/categories>categories</a> |
<a href=https://utsavdarlami.github.io/blogs/index.xml>feed</a> |
<a href=https://utsavdarlami.github.io/>home</a> |</p><article><h1>perceptron</h1><div style=float:left><time>Created Date : 2021 May 26</time></div><br><div style=float:left><time>Last Modified : 2021 July 13</time></div><hr><div>tags:
<a href=/tags/ml>ml</a>
<a href=/tags/deep-learning>deep learning</a></div><div style=float:left>categories:
<a href=/categories/uncategorized>uncategorized</a></div><div><hr><ul><li><p>References :</p><ul><li>Done Reading:<ul><li><a href=https://github.com/rasbt/stat453-deep-learning-ss20/blob/master/L03-perceptron/L03%5Fperceptron%5Fslides.pdf>Sebastian Raschka Perceptron Lecture Slide</a></li></ul></li></ul><ul><li>Reading:<ul><li>Tom Mitchell Lectures slides of chapter 4</li><li>Tom Mitchell, Machine Learning Chapter 4</li></ul></li><li>To Read:<ul><li><a href=https://www.wikiwand.com/en/Perceptron>https://www.wikiwand.com/en/Perceptron</a></li></ul></li></ul></li><li><p>Questions :</p></li></ul><hr><ul><li>A learning rule for the computational/mathematical neuron model</li><li>Rosenblatt, F. (1957). The perceptron, a perceiving and recognizing automaton. Project Para. Cornell Aeronautical Laboratory</li></ul><figure><img src=/blogs/ox-hugo/2021-06-02_21-10-22_screenshot.png alt="Figure 1: A perceptron" width=600 height=400><figcaption><p>Figure 1: A perceptron</p></figcaption></figure><ul><li>\(o(\overrightarrow{x}) = sgn(\overrightarrow{w} \cdot \overrightarrow{x})\) where,</li><li>\( sgn(y) = \begin{cases} 1 & \text{if y > 0} \\ -1 & \text{otherwise} \end{cases} \)</li><li>Space \(H\) of candidate hypotheses is<ul><li>\(H = \{ \overrightarrow{w} | \overrightarrow{w} \in \Re^{n+1} \}\)</li></ul></li></ul><h2 id=representations>Representations</h2><ul><li>A single layer perceptron can be used to represent many boolean functions<ul><li>AND<ul><li>\(w_{0}\) = -.8</li><li>\(w_{1}\) = \(w_{2}\) = .5</li></ul></li><li>OR<ul><li>\(w_{0}\) = -.3</li><li>\(w_{1}\) = \(w_{2}\) = .5</li></ul></li><li>NAND</li><li>NOR</li></ul></li><li>However XOR function cannot be represented by a single layer perceptron<ul><li>Since the single layer cannot linearly separate the training examples</li><li><code>In 1969, a famous book entitled Perceptrons by Marvin Minsky and Seymour Papert showed that it was impossible for these classes of network to learn an XOR function. It is often believed (incorrectly) that they also conjectured that a similar result would hold for a multi-layer perceptron network. However, this is not true, as both Minsky and Papert already knew that multi-layer perceptrons were capable of producing an XOR function. (See the page on Perceptrons (book) for more information.) Nevertheless, the often-miscited Minsky/Papert text caused a significant decline in interest and funding of neural network research. It took ten more years until neural network research experienced a resurgence in the 1980s</code> - <a href=https://www.wikiwand.com/en/Perceptron>Perceptron</a></li></ul></li></ul><figure><img src=/blogs/ox-hugo/2021-06-02_21-22-44_screenshot.png alt="Figure 2: XOR function" width=300 height=300><figcaption><p>Figure 2: XOR function</p></figcaption></figure><h2 id=learning-rule>Learning Rule</h2><figure><img src=/blogs/ox-hugo/2021-06-02_22-03-29_screenshot.png alt="Figure 3: Learning Rule, from Tom Mitchell Lectures" width=600 height=300><figcaption><p>Figure 3: Learning Rule, from Tom Mitchell Lectures</p></figcaption></figure><ul><li>converges only if (gauranteed to succeed):<ul><li>the data is linearly seperable</li><li>the \(\eta\) (eta) is sufficiently small</li></ul></li><li>Solution is <a href=2021-05-26--11-31-31Z--neural_network.md>Gradient Descent And the Delta Rule</a></li></ul><h2 id=geometric-intuition>Geometric Intuition</h2><figure><img src=/blogs/ox-hugo/2021-06-03_11-24-26_screenshot.png alt="Figure 4: Geometirx Intuition, from Sebastian Raschka Lectures" width=550 height=400><figcaption><p>Figure 4: Geometirx Intuition, from Sebastian Raschka Lectures</p></figcaption></figure></div><p style=font-size:1.65em;text-align:center><a href=/blogs/notes/2021-05-26--11-28-39z--regularization/>&#8678;</a>
<a href=/blogs/notes/2021-05-26--11-31-31z--neural_network/>&#8680;</a></p><hr><div class=bl-section><h4>Links to this note</h4><div class=backlinks><ul><li><a href=/blogs/notes/2021-05-26--11-31-31z--neural_network/>neural network</a></li></ul></div></div><hr><footer><main><h3>Comments</h3><script src=https://utteranc.es/client.js repo=utsavdarlami/blogs issue-term=pathname label=comment theme=github-light crossorigin=anonymous async></script></main></footer></article></main><footer></footer></body></html>