<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>notes for me on felladog</title><link>https://utsavdarlami.github.io/blogs/notes/</link><description>Recent content in notes for me on felladog</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Thu, 03 Jun 2021 16:52:00 +0545</lastBuildDate><atom:link href="https://utsavdarlami.github.io/blogs/notes/index.xml" rel="self" type="application/rss+xml"/><item><title>Inductive Bias</title><link>https://utsavdarlami.github.io/blogs/notes/2021-06-03-11-07-01z-inductive_bias/</link><pubDate>Thu, 03 Jun 2021 16:52:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/2021-06-03-11-07-01z-inductive_bias/</guid><description>References :
To Read: https://en.wikipedia.org/wiki/Inductive%5Fbias Questions :
The inductive bias of a learning algorithm is the set of assumptions that the learner uses to predict outputs given inputs that it has not encountered.</description></item><item><title>decision tree</title><link>https://utsavdarlami.github.io/blogs/notes/2021-06-03-11-01-27z-decision_tree/</link><pubDate>Thu, 03 Jun 2021 16:46:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/2021-06-03-11-01-27z-decision_tree/</guid><description> References :
Questions :</description></item><item><title>gradient</title><link>https://utsavdarlami.github.io/blogs/notes/2021-05-30-08-53-22z-gradient/</link><pubDate>Sun, 30 May 2021 14:38:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/2021-05-30-08-53-22z-gradient/</guid><description>References : Done Reading: Sebastian Raschka Gradient Descent Lecture Slide To Read: https://www.wikiwand.com/en/Gradient Questions : In vector calculus, the gradient is a multi-variable generalization of the derivative. Whereas the ordinary derivative of a function of a single variable is a scalar-valued function, the gradient of a function of several variables is a vector-valued function
Simply, derivative of multivariable function</description></item><item><title>neural network</title><link>https://utsavdarlami.github.io/blogs/notes/2021-05-26-11-31-31z-neural_network/</link><pubDate>Wed, 26 May 2021 17:16:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/2021-05-26-11-31-31z-neural_network/</guid><description>References : Done Reading: Sebastian Raschka Perceptron Lecture Slide Reading: Tom Mitchell Lectures slides of chapter 4 Tom Mitchell, Machine Learning Chapter 4 To Read: https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/ https://zzsza.github.io/data/2018/05/13/cs231n-backpropagation-and-neural-networks/ Questions : Biological Motivation Mimicing human brain Figure 1: biological neuron vs aritifical neuron from this post properties that match with the human brain Many neuron like threshold switching units Many weighted interconnections among units Highly parallel, distributed process Emphasis on tuning weights automatically Representations The ANNs can graphs be with many types of structures: acyclic or cyclic directed or undirected The backprop algorithm assumes ANN to have structure that corresponds to a directed graph, and possibly containing cyclees.</description></item><item><title>perceptron</title><link>https://utsavdarlami.github.io/blogs/notes/20210224173929-perceptron/</link><pubDate>Wed, 26 May 2021 17:16:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/20210224173929-perceptron/</guid><description>References :
Done Reading: Sebastian Raschka Perceptron Lecture Slide Reading: Tom Mitchell Lectures slides of chapter 4 Tom Mitchell, Machine Learning Chapter 4 To Read: https://www.wikiwand.com/en/Perceptron Questions :
A learning rule for the computational/mathematical neuron model Rosenblatt, F. (1957). The perceptron, a perceiving and recognizing automaton. Project Para. Cornell Aeronautical Laboratory Figure 1: A perceptron \(o(\overrightarrow{x}) = sgn(\overrightarrow{w} \cdot \overrightarrow{x})\) where, \( sgn(y) = \begin{cases} 1 &amp;amp; \text{if y &amp;gt; 0} \\ -1 &amp;amp; \text{otherwise} \end{cases} \) Space \(H\) of candidate hypotheses is \(H = \{ \overrightarrow{w} | \overrightarrow{w} \in \Re^{n+1} \}\) Representations A single layer perceptron can be used to represent many boolean functions AND \(w_{0}\) = -.</description></item><item><title>regularization</title><link>https://utsavdarlami.github.io/blogs/notes/2021-05-26-11-28-39z-regularization/</link><pubDate>Wed, 26 May 2021 17:13:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/2021-05-26-11-28-39z-regularization/</guid><description>References :
To read: https://www.wikiwand.com/en/Regularization%5F(mathematics) https://www.wikiwand.com/en/Early%5Fstopping Questions :
In machine learning and regularization is the process of adding information in order to solve an ill-posed problem or to prevent overfitting.
Early Stopping Form of regularization used to avoid overfitting when training a learner with an iterative method, such as gradient descent. Such methods update the learner so as to make it better fit the training data with each iteration.</description></item><item><title>activation function</title><link>https://utsavdarlami.github.io/blogs/notes/2021-05-22-10-42-02z-activation_function/</link><pubDate>Sat, 22 May 2021 16:27:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/2021-05-22-10-42-02z-activation_function/</guid><description>References : Reading: Tom Mitchell Lectures slides of chapter 4 Tom Mitchell, Machine Learning Chapter 4 Questions : Also known as squashing Function Figure 1: Activation Functions, from this post Sigmoid output is a non linear function of its inputs and is differentiable threshold function also called logistic function output ranges from 0 to 1 Figure 2: The sigmoid threshold unit, from Tom Mitchell Lectures Tanh \(f(x) = tanh(x) = \frac{(e^{x} - e^{-x})} {(e^{x} + e^{-x})}\)</description></item><item><title>Batch Normalization</title><link>https://utsavdarlami.github.io/blogs/notes/2021-03-06-02-21-33z-batch_normalization/</link><pubDate>Sat, 06 Mar 2021 08:06:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/2021-03-06-02-21-33z-batch_normalization/</guid><description>Acknowledgement : d2l.ai batch normalization L11.2 How BatchNorm Works - Sebastian Raschka https://twitter.com/karpathy/status/1013245864570073090 To read: Why is batch norm becoming so unpopular Questions : What is Normalization? Normalization A popular and effective technique that constantly accelerates the convergence of deep network. Normalizes hidden layer inputs. Helps with exploding/vanishing gradient problems. Can increase the training stability and convergence rate.</description></item><item><title>Recurrent Neural Networks</title><link>https://utsavdarlami.github.io/blogs/notes/20210213135051-recurrent_neural_networks/</link><pubDate>Tue, 02 Mar 2021 12:40:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/20210213135051-recurrent_neural_networks/</guid><description>Acknowledgement https://d2l.ai/chapter%5Frecurrent-neural-networks/index.html
https://github.com/fastai/fastbook/blob/master/12%5Fnlp%5Fdive.ipynb
https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks
https://colah.github.io/posts/2015-08-Understanding-LSTMs/ [Mainly LSTM]
https://deeplearning.cs.cmu.edu/S20/document/recitation/recitation-7.pdf
http://cs231n.stanford.edu/slides/2017/cs231n%5F2017%5Flecture10.pdf
https://www.youtube.com/watch?v=6niqTuYFZLQ
http://ethen8181.github.io/machine-learning/deep%5Flearning/rnn/1%5Fpytorch%5Frnn.html#Recurrent-Neural-Network-(RNN) [Main reference]
https://medium.com/ecovisioneth/building-deep-multi-layer-recurrent-neural-networks-with-star-cell-2f01acdb73a7 [Multi Layer]
https://towardsdatascience.com/pytorch-basics-how-to-train-your-neural-net-intro-to-rnn-cb6ebc594677
https://www.jeremyjordan.me/introduction-to-recurrent-neural-networks/
https://towardsdatascience.com/pytorch-basics-how-to-train-your-neural-net-intro-to-rnn-cb6ebc594677
Recurrent neural networks (RNNs) are designed to better handle sequential information. (text and stocks)
RNNs introduce state variables(hidden state) to store past information, together with the current inputs, to determine the current outputs.</description></item></channel></rss>