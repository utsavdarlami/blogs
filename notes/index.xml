<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Notes for me on felladog</title><link>https://utsavdarlami.github.io/blogs/notes/</link><description>Recent content in Notes for me on felladog</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Fri, 04 Jun 2021 19:53:00 +0545</lastBuildDate><atom:link href="https://utsavdarlami.github.io/blogs/notes/index.xml" rel="self" type="application/rss+xml"/><item><title>concept learning</title><link>https://utsavdarlami.github.io/blogs/notes/2021-06-04-14-08-26z-concept_learning/</link><pubDate>Fri, 04 Jun 2021 19:53:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/2021-06-04-14-08-26z-concept_learning/</guid><description>References : Reading: Tom Mitchell Lectures slides of chapter 2 Tom Mitchell, Machine Learning Chapter 2 To Read: Candidate elimination algorithm with an example Questions : Inferring boolean valued function from training examples of its input and outputs. A problem of searching through a predefined space of potential hypotheses for the hypothesis that best fits the training examples Example Traget concept or function: Concept or function to be learned.</description></item><item><title>Inductive Bias</title><link>https://utsavdarlami.github.io/blogs/notes/2021-06-03-11-07-01z-inductive_bias/</link><pubDate>Thu, 03 Jun 2021 16:52:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/2021-06-03-11-07-01z-inductive_bias/</guid><description>References :
Reading: Tom Mitchell Lectures slides of chapter 2 Tom Mitchell, Machine Learning Chapter 2 To Read: https://en.wikipedia.org/wiki/Inductive%5Fbias Questions :
The inductive bias of a learning algorithm is the set of assumptions that the learner uses to predict outputs given inputs that it has not encountered.
Consider a concept learning algorithm L for the set of instances X.</description></item><item><title>decision tree</title><link>https://utsavdarlami.github.io/blogs/notes/2021-06-03-11-01-27z-decision_tree/</link><pubDate>Thu, 03 Jun 2021 16:46:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/2021-06-03-11-01-27z-decision_tree/</guid><description>References :
Reading : Tom Mitchell Lectures slides of chapter 3 Tom Mitchell, Machine Learning Chapter 3 ID3 algorithm complete solution Reddit Post Link Drive Pdf Link To Read : Chapter 3 — Decision Tree Learning — Part 1 Questions :
Approximating discrete-valued fucntions robust to noisy data capable of learning disjunctive expressions search a completely expressive hypothesis space and thus avoid the difficulties of restricted hypothesis spaces.</description></item><item><title>edge filters</title><link>https://utsavdarlami.github.io/blogs/notes/2021-05-31-03-54-38z-edge_filters/</link><pubDate>Mon, 31 May 2021 09:39:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/2021-05-31-03-54-38z-edge_filters/</guid><description>References :
Done Reading Reading https://nptel.ac.in/courses/106/106/106106224/ [IIT Madras Week 2 part 1] Cornell CV lecture on Edge https://szeliski.org/Book/ [Book Chapter 3] https://www.wikiwand.com/en/Sobel%5Foperator Joseph Redmon Lectures https://evergreenllc2020.medium.com/fundamentals-of-image-gradients-and-edge-detection-b093662ade1b To read: Questions :
Criteria for a good edge detector Good detection find all real edges, ignoring noise or other artifacts Good Localization detect edges as close as possible to true edges Single Response return one point only for each true edge point from skimage.</description></item><item><title>gradient</title><link>https://utsavdarlami.github.io/blogs/notes/2021-05-30-08-53-22z-gradient/</link><pubDate>Sun, 30 May 2021 14:38:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/2021-05-30-08-53-22z-gradient/</guid><description>References : Done Reading: Sebastian Raschka Gradient Descent Lecture Slide To Read: https://www.wikiwand.com/en/Gradient Questions : In vector calculus, the gradient is a multi-variable generalization of the derivative. Whereas the ordinary derivative of a function of a single variable is a scalar-valued function, the gradient of a function of several variables is a vector-valued function
Simply, derivative of multivariable function</description></item><item><title>image filtering</title><link>https://utsavdarlami.github.io/blogs/notes/2021-05-28-12-02-59z-image_convolution/</link><pubDate>Fri, 28 May 2021 17:47:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/2021-05-28-12-02-59z-image_convolution/</guid><description>References : Done Reading Reading https://vincmazet.github.io/ftip/convolution.html https://nptel.ac.in/courses/106/106/106106224/ [IIT Madras] Cornell CV Lecture on Filter https://szeliski.org/Book/ [Book Chapter 3] Non-local means denoising for preserving textures To read: https://www.cs.tut.fi/~foi/papers/ICIP2019%5FYmir.pdf [BM3D] https://stackoverflow.com/questions/40596026/what-does-entropy-mean-in-this-context Questions : Modify image pixels based on some function of a local neighbourhood of each pixel.
Why filter a image To sharpen, smooth, intensify or enhance a imag Linear filters Output pixel’s value is determined as a weighted sum of input pixel values within a small neighborhood N,</description></item><item><title>neural network</title><link>https://utsavdarlami.github.io/blogs/notes/2021-05-26-11-31-31z-neural_network/</link><pubDate>Wed, 26 May 2021 17:16:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/2021-05-26-11-31-31z-neural_network/</guid><description>References : Done Reading: Sebastian Raschka Perceptron Lecture Slide Reading: Tom Mitchell Lectures slides of chapter 4 Tom Mitchell, Machine Learning Chapter 4 To Read: https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/ https://zzsza.github.io/data/2018/05/13/cs231n-backpropagation-and-neural-networks/ Questions : Biological Motivation Mimicing human brain Figure 1: biological neuron vs aritifical neuron from this post properties that match with the human brain Many neuron like threshold switching units Many weighted interconnections among units Highly parallel, distributed process Emphasis on tuning weights automatically Representations The ANNs can graphs be with many types of structures: acyclic or cyclic directed or undirected The backprop algorithm assumes ANN to have structure that corresponds to a directed graph, and possibly containing cyclees.</description></item><item><title>perceptron</title><link>https://utsavdarlami.github.io/blogs/notes/20210224173929-perceptron/</link><pubDate>Wed, 26 May 2021 17:16:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/20210224173929-perceptron/</guid><description>References :
Done Reading: Sebastian Raschka Perceptron Lecture Slide Reading: Tom Mitchell Lectures slides of chapter 4 Tom Mitchell, Machine Learning Chapter 4 To Read: https://www.wikiwand.com/en/Perceptron Questions :
A learning rule for the computational/mathematical neuron model Rosenblatt, F. (1957). The perceptron, a perceiving and recognizing automaton. Project Para. Cornell Aeronautical Laboratory Figure 1: A perceptron \(o(\overrightarrow{x}) = sgn(\overrightarrow{w} \cdot \overrightarrow{x})\) where, \( sgn(y) = \begin{cases} 1 &amp;amp; \text{if y &amp;gt; 0} \\ -1 &amp;amp; \text{otherwise} \end{cases} \) Space \(H\) of candidate hypotheses is \(H = \{ \overrightarrow{w} | \overrightarrow{w} \in \Re^{n+1} \}\) Representations A single layer perceptron can be used to represent many boolean functions AND \(w_{0}\) = -.</description></item><item><title>regularization</title><link>https://utsavdarlami.github.io/blogs/notes/2021-05-26-11-28-39z-regularization/</link><pubDate>Wed, 26 May 2021 17:13:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/2021-05-26-11-28-39z-regularization/</guid><description>References :
To read: https://www.wikiwand.com/en/Regularization%5F(mathematics) https://www.wikiwand.com/en/Early%5Fstopping Questions :
In machine learning and regularization is the process of adding information in order to solve an ill-posed problem or to prevent overfitting.
Early Stopping Form of regularization used to avoid overfitting when training a learner with an iterative method, such as gradient descent. Such methods update the learner so as to make it better fit the training data with each iteration.</description></item><item><title>activation function</title><link>https://utsavdarlami.github.io/blogs/notes/2021-05-22-10-42-02z-activation_function/</link><pubDate>Sat, 22 May 2021 16:27:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/2021-05-22-10-42-02z-activation_function/</guid><description>References : Reading: Tom Mitchell Lectures slides of chapter 4 Tom Mitchell, Machine Learning Chapter 4 Questions : Also known as squashing Function Figure 1: Activation Functions, from this post Sigmoid output is a non linear function of its inputs and is differentiable threshold function also called logistic function output ranges from 0 to 1 Figure 2: The sigmoid threshold unit, from Tom Mitchell Lectures Tanh \(f(x) = tanh(x) = \frac{(e^{x} - e^{-x})} {(e^{x} + e^{-x})}\)</description></item><item><title>text-preprocessing</title><link>https://utsavdarlami.github.io/blogs/notes/2021-03-31-07-12-59z-text_preprocessing/</link><pubDate>Wed, 31 Mar 2021 12:57:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/2021-03-31-07-12-59z-text_preprocessing/</guid><description>References : [Book] Builiding the ML systems with python http://tfidf.com/ Questions : Bag of Words Bag of word approach, it totally ignores the order of words and simply uses word counts as their basis. For each word in the post, its occurrence is counted and noted in a vector. Not surprisingly, this step is also called vectorization. The vector is typically huge as it contains as many elements as words occur in the whole dataset.</description></item><item><title>entropy</title><link>https://utsavdarlami.github.io/blogs/notes/2021-03-06-03-56-06z-entropy/</link><pubDate>Sat, 06 Mar 2021 09:41:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/2021-03-06-03-56-06z-entropy/</guid><description>References :
https://www.wikiwand.com/en/Entropy%5F(information%5Ftheory) Entropy in image, scikit-image [3] https://bricaud.github.io/personal-blog/entropy-in-decision-trees/
Questions :
What is the point of entropy in decision tree? Entropy is a measure of disorder. A high entropy is essentially saying that the data is scattered around while a low entropy means that nearly all the data is the same. The highest entropy one can achieve is 1 and lowest entropy one can achieve is 0.</description></item><item><title>Batch Normalization</title><link>https://utsavdarlami.github.io/blogs/notes/2021-03-06-02-21-33z-batch_normalization/</link><pubDate>Sat, 06 Mar 2021 08:06:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/2021-03-06-02-21-33z-batch_normalization/</guid><description>Acknowledgement : d2l.ai batch normalization L11.2 How BatchNorm Works - Sebastian Raschka https://twitter.com/karpathy/status/1013245864570073090 To read: Why is batch norm becoming so unpopular Questions : What is Normalization? Normalization A popular and effective technique that constantly accelerates the convergence of deep network. Normalizes hidden layer inputs. Helps with exploding/vanishing gradient problems. Can increase the training stability and convergence rate.</description></item><item><title>Recurrent Neural Networks</title><link>https://utsavdarlami.github.io/blogs/notes/20210213135051-recurrent_neural_networks/</link><pubDate>Tue, 02 Mar 2021 12:40:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/20210213135051-recurrent_neural_networks/</guid><description>Acknowledgement https://d2l.ai/chapter%5Frecurrent-neural-networks/index.html
https://github.com/fastai/fastbook/blob/master/12%5Fnlp%5Fdive.ipynb
https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks
https://colah.github.io/posts/2015-08-Understanding-LSTMs/ [Mainly LSTM]
https://deeplearning.cs.cmu.edu/S20/document/recitation/recitation-7.pdf
http://cs231n.stanford.edu/slides/2017/cs231n%5F2017%5Flecture10.pdf
https://www.youtube.com/watch?v=6niqTuYFZLQ
http://ethen8181.github.io/machine-learning/deep%5Flearning/rnn/1%5Fpytorch%5Frnn.html#Recurrent-Neural-Network-(RNN) [Main reference]
https://medium.com/ecovisioneth/building-deep-multi-layer-recurrent-neural-networks-with-star-cell-2f01acdb73a7 [Multi Layer]
https://towardsdatascience.com/pytorch-basics-how-to-train-your-neural-net-intro-to-rnn-cb6ebc594677
https://www.jeremyjordan.me/introduction-to-recurrent-neural-networks/
https://towardsdatascience.com/pytorch-basics-how-to-train-your-neural-net-intro-to-rnn-cb6ebc594677
Recurrent neural networks (RNNs) are designed to better handle sequential information. (text and stocks)
RNNs introduce state variables(hidden state) to store past information, together with the current inputs, to determine the current outputs.</description></item><item><title>ML Model Evaluation</title><link>https://utsavdarlami.github.io/blogs/notes/20210224162319-model_evaluations_score/</link><pubDate>Wed, 24 Feb 2021 16:23:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/20210224162319-model_evaluations_score/</guid><description>references To Read: https://towardsdatascience.com/handling-imbalanced-datasets-in-machine-learning-7a0e84220f28 questions ml model evaluation methods
Classfication True Positive (TP) Positive class correctly labeled/predicted False Negative (FN) Positive class incorrectly labeled/predicted False Positive (FP) Negative class incorrectly labeled/predicted True Negative (TN) Negative class correctly labeled/predicted Accuracy It is simply a ratio of correctly predicted observation to the total observations.</description></item><item><title>image segmentation</title><link>https://utsavdarlami.github.io/blogs/notes/20210224161655-image_segmentation/</link><pubDate>Wed, 24 Feb 2021 16:16:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/20210224161655-image_segmentation/</guid><description>references: https://www.youtube.com/watch?v=AZr64OxshLo https://www.youtube.com/watch?v=0FmNxqLFeYo questions : Using Classical Approach Histogram Based Segmentation thresholding Random Walker Segmentation Watershed Segmentation Graph based segmentation &amp;lt;/home/felladog/Downloads/CV_Nptel/W2/DL4CV_Week02_Part05.pdf&amp;gt; https://youtu.be/0HbRnFTOOms?t=680 Using Unsupervised Approach Gaussian Mixture Model Kmeans Clustering Using Deep Learning UNET Image Segmentation Loss/Evaluation Method Problem with other common metrics We need metrics that target pixels in foreground Intersection over union (Jaccard index) sklearn jaccard module</description></item><item><title>machine learning</title><link>https://utsavdarlami.github.io/blogs/notes/20210119123811-machine_learning/</link><pubDate>Sun, 17 Jan 2021 17:28:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/20210119123811-machine_learning/</guid><description>references Reading https://www.cs.cmu.edu/~tom/mlbook.html To Read https://rentruewang.github.io/learning-machine/intro.html https://medium.com/octavian-ai/how-to-get-started-with-machine-learning-on-graphs-7f0795c83763 https://medium.datadriveninvestor.com/3-steps-introduction-to-machine-learning-and-design-of-a-learning-system-bd12b65aa50c https://chrisalbon.com/ Interpretable Machine Learning A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.
Machine learning is a subfield of AI that studies the ability to improve performance based on experience.</description></item></channel></rss>