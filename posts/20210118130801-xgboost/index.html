<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>xgboost</title><meta name=description content="felladog"><meta name=author content="Utsav Darlami"><link rel=stylesheet href=/blogs/css/style.css><link rel=stylesheet href=/blogs/css/syntax.css><link rel=stylesheet href=/blogs/css/toc.css><link rel=stylesheet href=/blogs/katex/katex.min.css><script defer src=/blogs/katex/katex.min.js></script><script defer src=/blogs/katex/contrib/auto-render.min.js onload=renderMathInElement(document.body)></script></head><body><header></header><main><div class=toc><nav id=TableOfContents><ul><li><a href=#for-regression>For Regression</a></li><li><a href=#for-classification>For Classification</a></li><li><a href=#mathematical-details>Mathematical Details</a></li></ul></nav><a href=# class=back-to-top>Back to top</a></div><script src=https://utsavdarlami.github.io/blogs/js/libs/jquery/3.3.1/jquery.slim.min.js></script><script>(function(){var a=$('#TableOfContents'),b;if(a.length>0){b=$(window);function c(){var e=b.scrollTop(),f=$('.body h1, .body h2, .body h3, .body h4, .body h5, .body h6'),c="",d;if(f.each(function(b,a){a=$(a),a.offset().top-10<=e&&(c=a.attr('id'))}),d=a.find('a.current'),d.length==1&&d.eq(0).attr('href')=='#'+c)return!0;d.each(function(b,a){$(a).removeClass('current').siblings('ul').hide()}),a.find('a[href="#'+c+'"]').parentsUntil('#TableOfContents').each(function(b,a){$(a).children('a').addClass('current').siblings('ul').show()})}b.on('scroll',c),$(document).ready(function(){a.find('a').parent('li').find('ul').hide(),c(),document.getElementsByClassName('toc')[0].style.display=''})}})()</script><article><p align=right><a href=https://utsavdarlami.github.io/>home</a> |
<a href=https://utsavdarlami.github.io/blogs/>blogs</a> |
<a href=https://utsavdarlami.github.io/blogs/about>about</a> |
<a href=https://github.com/utsavdarlami/>github</a> |
<a href=https://utsavdarlami.github.io/blogs/tags>tags</a> |
<a href=https://utsavdarlami.github.io/blogs/categories>categories</a> |
<a href=https://utsavdarlami.github.io/blogs/index.xml>feed</a></p><h1>xgboost</h1><div style=float:left><time>2021 January 18</time></div><hr><div>tags:
<a href=/blogs/tags/ensemble>ensemble</a>
<a href=/blogs/tags/boosting>boosting</a>
<a href=/blogs/tags/regression>regression</a>
<a href=/blogs/tags/classfication>classfication</a>
<a href=/blogs/tags/gain>gain</a></div><div style=float:left>categories:
<a href=/blogs/categories/machine-learning>machine learning</a></div><div><hr><ul><li>publish date : 18 jan 2021</li><li>review date : 28 feb 2021</li><li>references :<ul><li><a href="https://www.youtube.com/watch?v=OtD8wVaFm6E">XGBoost Part 1 (of 4): Regression</a></li><li><a href="https://www.youtube.com/watch?v=ZVFeW798-2I">XGBoost Part 3 Math Details Stat Quests</a></li></ul></li><li>Questions :<ul><li>What is log odds, from log odds to probability</li></ul></li></ul><hr><ul><li>Machine Learning Algorithm for regression and classification problems.</li></ul><h2 id=for-regression>For Regression</h2><ul><li>Similarity score = \(\frac{ { \text{Sum of residuals, Squared} } } {\text{number of residuals}+ \lambda} }\)</li><li><strong>Gain</strong> to determine how to split the data (Gain should be high)</li><li>Gain = \(Left_{similarity} + Right_{similarity} - Root_{similarity}\)</li><li>Pruning the tree by calculating the difference between Gain and the tree complexity Parameter, \(\gamma\) (Gamma).</li><li>Gain - \(\gamma\) = if positive then no pruning else prune the tree</li><li>Calculate the output values for the remaining leaves, Output Value = \(\frac{ { \text{Sum of residuals} } } {\text{number of residuals}+ \lambda} }\)</li><li>\(\lambda\) Lambda is the regularization parameter</li><li>\(\lambda\) > 0, results in more pruning, by shrinking the similarity scores and results in smaller output values for the leaves</li></ul><h2 id=for-classification>For Classification</h2><ul><li><p>For Classification Similarity score = \(\frac{ { \text{Sum of residuals, Squared} } } {\sum\text{[Previous Probability * (1 - Prev Probability)]}+ \lambda} }\)</p></li><li><p>Cover determines the minimum number of Residuals in each leaf.</p></li><li><p>Cover = Similarity Score - \(\lambda\)</p></li><li><p>In Classification, Cover = \(\sum{\text{[Previous Probability * (Prev Probability - 1)]}\)</p></li><li><p>In Regression, Cover = \(\text{Number of Residuals}\)</p></li><li><p>For Classification Output Value = \(\frac{ { \text{Sum of residuals} } } {\sum\text{[Previous Probability * (1- Prev Probability)]}+ \lambda} }\)</p></li></ul><h2 id=mathematical-details>Mathematical Details</h2><ul><li></li></ul></div><p style=font-size:1.25em;text-align:center><a href=/blogs/posts/20210224161655-image_segmentation/>&#8680;</a></p></article><aside><div><div><h3>OTHER POSTS</h3></div><div><ul><li><a href=/blogs/posts/2021-02-28-12-07-51z-loading_dataset_on_colab/>loading dataset on colab</a></li><li><a href=/blogs/posts/2021-02-28-11-34-36z-blog_using_org_and_hugo/>blog using org and hugo</a></li><li><a href=/blogs/posts/20210204193730-pytorch_learning_path/>pytorch learning path</a></li></ul></div></div></aside></main><footer></footer></body></html>