<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>backpropagation on felladog</title><link>https://utsavdarlami.github.io/blogs/tags/backpropagation/</link><description>Recent content in backpropagation on felladog</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Wed, 26 May 2021 17:16:00 +0545</lastBuildDate><atom:link href="https://utsavdarlami.github.io/blogs/tags/backpropagation/index.xml" rel="self" type="application/rss+xml"/><item><title>neural network</title><link>https://utsavdarlami.github.io/blogs/notes/2021-05-26--11-31-31z--neural_network/</link><pubDate>Wed, 26 May 2021 17:16:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/2021-05-26--11-31-31z--neural_network/</guid><description>References : Done Reading: Sebastian Raschka Perceptron Lecture Slide Reading: Tom Mitchell Lectures slides of chapter 4 Tom Mitchell, Machine Learning Chapter 4 To Read: https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/ https://zzsza.github.io/data/2018/05/13/cs231n-backpropagation-and-neural-networks/ Questions : Biological Motivation Mimicing human brain Figure 1: biological neuron vs aritifical neuron from this post properties that match with the human brain Many neuron like threshold switching units Many weighted interconnections among units Highly parallel, distributed process Emphasis on tuning weights automatically Representations The ANNs can graphs be with many types of structures: acyclic or cyclic directed or undirected The backprop algorithm assumes ANN to have structure that corresponds to a directed graph, and possibly containing cyclees.</description></item></channel></rss>