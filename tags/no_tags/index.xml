<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>no_tags on felladog</title><link>https://utsavdarlami.github.io/blogs/tags/no_tags/</link><description>Recent content in no_tags on felladog</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Thu, 27 May 2021 11:05:00 +0545</lastBuildDate><atom:link href="https://utsavdarlami.github.io/blogs/tags/no_tags/index.xml" rel="self" type="application/rss+xml"/><item><title>image thresholding</title><link>https://utsavdarlami.github.io/blogs/notes/2021-05-27--05-20-06z--thresholding/</link><pubDate>Thu, 27 May 2021 11:05:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/2021-05-27--05-20-06z--thresholding/</guid><description>References :
https://muthu.co/otsus-method-for-image-thresholding-explained-and-implemented/ Questions :
Can be used for image segmentation
multi-otsu thresholding The multi-Otsu threshold is a thresholding algorithm that is used to separate the pixels of an input image into several different classes, each one obtained according to the intensity of the gray levels within the image Multi-Otsu calculates several thresholds based on the number supplied by the user. for multi classes import numpy as np import matplotlib.</description></item><item><title>regularization</title><link>https://utsavdarlami.github.io/blogs/notes/2021-05-26--11-28-39z--regularization/</link><pubDate>Wed, 26 May 2021 17:13:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/2021-05-26--11-28-39z--regularization/</guid><description>References :
To read: https://www.wikiwand.com/en/Regularization%5F(mathematics) https://www.wikiwand.com/en/Early%5Fstopping Questions :
In machine learning and regularization is the process of adding information in order to solve an ill-posed problem or to prevent overfitting.
Early Stopping Form of regularization used to avoid overfitting when training a learner with an iterative method, such as gradient descent. Such methods update the learner so as to make it better fit the training data with each iteration. Up to a point, this improves the learner&amp;rsquo;s performance on validation set.</description></item><item><title>activation function</title><link>https://utsavdarlami.github.io/blogs/notes/2021-05-22--10-42-02z--activation_function/</link><pubDate>Sat, 22 May 2021 16:27:00 +0545</pubDate><guid>https://utsavdarlami.github.io/blogs/notes/2021-05-22--10-42-02z--activation_function/</guid><description>References : Reading: Tom Mitchell Lectures slides of chapter 4 Tom Mitchell, Machine Learning Chapter 4 Questions : Also known as squashing Function Figure 1: Activation Functions, from this post Sigmoid output is a non linear function of its inputs and is differentiable threshold function also called logistic function output ranges from 0 to 1 Figure 2: The sigmoid threshold unit, from Tom Mitchell Lectures Tanh \(f(x) = tanh(x) = \frac{(e^{x} - e^{-x})} {(e^{x} + e^{-x})}\)</description></item></channel></rss>